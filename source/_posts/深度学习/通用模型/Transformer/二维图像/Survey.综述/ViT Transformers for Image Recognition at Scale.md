---
title: 一张图片等于 $16\times16$ 个单词：用于大规模图像识别的 Transformers
excerpt: 虽然 Transformer 体系结构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在当前的视觉模型中，注意力要么与卷积网络结合应用，要么替换卷积网络的某些组成部分，同时保持它们的整体结构。我们证明了这种对 CNN 的依赖不是必要的，而一个直接应用于图像块序列的纯 Transformer 可以很好地执行图像分类任务。当对大量数据进行预训练，并迁移到多个中型或小型图像识别基准测试（ImageNet、CIFAR-100、VTAB等）时，与最先进的卷积网络相比，视觉 Transformer （Vision Transformer, ViT）获得了良好的结果，同时需要更少的计算资源来训练[^注1]。
categories:
  - 图像特征
tags:
  - 图像特征
  - 计算机视觉
  - Self-Attention
  - Transformer
date: 
updated: 
toc: true
typora-root-url: D:\Projects\Github\zhuyuanxiang\hexo_pages\hexo-starter\source\_posts\
---

# 一张图片等于 $16\times16$ 个单词：用于大规模图像识别的 Transformers

Bibtex：Dosovitskiy A, Beyer L, Kolesnikov A,等 An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020.

[原始论文](https://arxiv.org/pdf/2010.11929.pdf)

[中文翻译]()

[原始代码](https://github.com/google-research/vision_transformer)

[PyTorch代码](https://github.com/lucidrains/vit-pytorch)

# 摘要

虽然 Transformer 体系结构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在当前的视觉模型中，注意力要么与卷积网络结合应用，要么替换卷积网络的某些组成部分，同时保持它们的整体结构。我们证明了这种对 CNN 的依赖不是必要的，而一个直接应用于图像块序列的纯 Transformer 可以很好地执行图像分类任务。当对大量数据进行预训练，并迁移到多个中型或小型图像识别基准测试（ImageNet、CIFAR-100、VTAB等）时，与最先进的卷积网络相比，视觉 Transformer （Vision Transformer, ViT）获得了良好的结果，同时需要更少的计算资源来训练[^注1]。

[^注1]: 微调代码和预训练模型[参考网址](https://github.com/ google-research/vision_transformer)

## 关键词

Transformer，视觉（Vision），视觉 Transformer（Vision Transformer，ViT），混合 Transformer（Hybrid Transformer，HiT），自然语言处理（Natural Language Processing, NLP），多层感知机（MLP），图像块（Image Patch），卷积神经网络（CNN），注意力（Attention），自注意力（Self-Attention，SA），多头自注意力（Multi-head Self-Attention，MSA）

# Ch01 简介

基于自注意力的架构，特别是 Transformer （Vaswani等，2017），已经成为 NLP 的首选模型。主要模型就是在大型文本语料库上进行预训练，然后在较小的特定任务数据集上进行微调（Devlin等，2019）。由于Transformer 的计算效率和可伸缩性，训练具有超过 $100B$ 参数的超大尺寸模型已经成为可能（Brown等，2020；Lepikhin等，2020）。随着模型和数据集的增长，仍然没有性能饱和的迹象。

然而，在计算机视觉中，卷积架构仍然占主导地位（LeCun等，1989；Krizhevsky等，2012；He等，2016）。受NLP成功的启发，多个研究尝试将类似 CNN 的架构与自注意力结合起来（Wang等，2018；Carion等，2020），部分工作则完全取代卷积（Ramachandran等，2019；Wang等，2020a）。后一种模型虽然在理论上是有效的，但由于使用了特定的注意力模式，不能在现代的硬件加速器上有效地进行规模扩展。因此，在大规模图像识别中，经典的 ResNet 类架构仍然是最先进的技术（Mahajan等，2018；Xie等，2020；Kolesnikov等，2020）。

受 NLP 中 Transformer 缩放成功的启发，我们尝试以最少的合理修改将标准 Transformer 直接应用于图像。为此，我们将一个图像分割成一片片图像块，然后将这些图像块的线性嵌入当作序列输入到一个 Transformer 中。图像块的处理方式与NLP应用程序中的标记（单词）的处理方式相同。我们以有监督的方式训练该模型进行图像分类。

当在中等大小的数据集，如：没有强正则化的 ImageNet 上进行训练，这些模型的精度比相当大小的 ResNets 低几个百分点。这种看似令人沮丧的结果是可以预料到的：Transformer 缺乏 CNN 所固有的归纳偏差的一些能力，如：平移等变性和局部性，因此在不充分的数据量上训练时不能很好地泛化。

然而，如果模型在更大的数据集（14M-300M 张图像）上进行训练，局面就会发生变化。我们发现，大规模数据集的训练胜过了归纳偏差。当预训练的数据集规模充分，并且使用更少量的数据点迁移到任务时，ViT 模型获得了优秀的结果。当在 ImageNet-21k 公共数据集或 JFT-300M 户内数据集上进行预训练时，ViT 在多个图像识别基准上接近或击败了最新水平。特别是，ViT 的最佳模型在 ImageNet 上准确率为 $88.55\%$，在 ImageNet-ReaL 上准确率为 $90.72\%$，在 CIFAR-100 上准确率为 $94.55\%$，在 VTAB 的 $19$ 个任务上准确率为 $77.63\%$。

# Ch02 相关工作

Transformer 是由 Vaswani等（2017）提出的用于机器翻译的方法，并已成为许多 NLP 任务中的最优方法。基于大型 Transformer 的模型通常在大型语料库上进行预训练，然后对手头的任务进行微调： BERT（Devlin等，2019）使用了去噪自监督的训练前任务，而 GPT 系列工作使用语言建模作为其预训练任务（Radford等，2018；2019；Brown等，2020）。

自注意力在图像上的初级应用需要每个像素与其他像素计算关注度，其代价是像素数的二次方，所以就无法扩展到现实中的图像输入尺寸。因此，为了将 Transformer 应用于图像处理的上下文中，过去已经尝试过几种类似的方法。Parmar等（2018）只将自注意力应用在每个查询像素的局部邻域中，而不是全局中。这种局部多头点积自注意力模块可以完全地取代卷积（Hu等，2019；Ramachandran等，2019；Zhao等，2020）。在另一项工作中，稀疏 Transformer （Child等，2019）通过在全局自注意力应用可扩展的逼近，从而满足图像尺寸缩放的需要。还有一种可选的方式是将可以缩放的注意力应用于不同大小的图像块中（Weissenborn等，2019），在极端情况下仅沿着独立的轴应用注意力（Ho等，2019；Wang等，2020a）。许多特定的注意力架构在计算机视觉任务上显示出了值得期望的结果，但是需要复杂的工程实现才能在硬件加速器上有效地实现。

与我们最相关的是 Cordonnier等（2020）的模型，该模型从输入图像中提取大小为 $2×2$ 的图像块，并在顶部应用完全的自注意力。这个模型与 ViT 非常相似，但我们的工作进一步证明，大规模的预训练使普通 Transformer 能够与最先进的 CNN 竞争（甚至更好）。此外，Cordonnier 等（2020）使用了 $2×2$ 像素的小尺寸图像块，这使得该模型仅适用于低分辨率图像，而我们也能处理中分辨率图像。

也有很多人对 CNN 与自注意力相结合的形式感兴趣，例如：用于图像分类的特征图增强（Bello等，2019）；或者使用自注意力进一步处理 CNN 的输出，例如：目标检测（Hu等，2018；Carion等，2020)，视频处理（Wang等，2018；Sun等，2019)，图像分类（Wu等，2020)，无监督对象发现（Locatello等，2020)，以及统一的文本-视觉任务（Chen等，2020c；Lu等，2019；Li等，2019)。

另一个最近的相关模型是图像GPT（iGPT）（Chen等，2020a），它在降低图像分辨率和颜色空间后，将Transformer 应用于图像像素。该模型以无监督的方式训练生成模型，然后通过对表示结果进行微调或线性探测增强分类的性能，在 ImageNet 上达到 $72\%$ 的最大准确率。

我们的工作增加了更多论文的收集，除了标准的 ImageNet 数据集外，还探索了更大尺度的图像识别。使用了额外的数据源后，可以在标准的基准上实现最先进的结果（Mahajan等，2018；Touvron等，2019；Xie等，2020）。此外，Sun等（2017）研究了 CNN 的性能如何随着数据集的大小而变化，Kolesnikov等（2020）；Djolonga等（2020）在 ImageNet-21k 和 JFT-300M 这类大型数据集上对 CNN 的迁移学习进行了实证探索。我们也关注了这两个数据集，但是在训练 Transformer 来代替基于 ResNet 模型时使用的是之前工作中的数据集。

# Ch03 具体方法

在模型设计中，我们尽可能遵循原始的 Transformer （Vaswani等，2017）。这种有意识地简单设置的优点是，可伸缩的 NLP Transformer 架构及其高效的实现几乎可以开箱即用。

![image-20231227112845167](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231227112845167.png)

图1：模型预览。我们将一个图像分割成固定大小的图像块，线性嵌入每个图像块，再添加位置嵌入，并将得到的向量序列提供给一个标准的Transformer 编码器。为了进行分类，我们使用了标准的方法：在序列中添加额外的、可学习的“分类标记”。Transformer 编码器的图示是受到 Vaswani等（2017）的启发。

## 3.1. 视觉 Transformer

如图1所示，标准的 Transformer 的输入是 1D 序列，序列是由标记的嵌入式编码组成。而处理二维图像，我们需要将图像 $\mathbf{x}\in\mathbb{R}^{H\times W\times C}$ 变形为一个摊平的 2D 图像块 $\mathbf{x}_p\in\mathbb{R}^{N\times(P^2\cdot C)}$ 序列，其中的$(H,W)$是原始图像的分辨率，$C$是通道数，$P^2$是每个图像块的分辨率，$N=HW/P^2$ 是图像块的数量，这个 $N$ 也是 Transformer 的输入序列的有效长度。Transformer 在所有层中使用的不变的隐向量大小为 $D$，所以我们将图像块摊平，并且通过可训练的线性投影映射到 $D$ 维，详见等式(1)。我们将这个投影的输出称为**图像块嵌入**。

与BERT中的 `[class]` 标记类似，我们在嵌入的图像块序列中前置了一个可以学习的嵌入编码（$\mathbf{z}^0_0=\mathbf{x}_{\text{class}}$），其在 Transformer 编码器（$\mathbf{z}^0_L$）的输出中作为图像表示 $\mathbf{y}$ 存在，详见等式(4)。在预训练和微调过程中，一个分类头被附加在 $\mathbf{z}^0_L$上。分类头在预训练阶段由一个 MLP 实现（包含一个隐藏层），在微调阶段则由一个单一的线性层实现。

位置嵌入被添加到图像块嵌入中，以保留位置信息。我们仅使用标准的可学习的 1D 位置嵌入，因为使用了更先进的 2D 感知位置嵌入并没有观察到性能地显著提高，详见附录[D.4]。最终得到的嵌入向量序列作为编码器的输入。

Transformer 编码器（Vaswani等，2017）由等式(2)中的多头自注意力（Multiheaded Self-Attention，MSA，见附录[A]）和等式(3)中的 MLP 块交替组成。Wang等（2019）在每个块之前应用层归一化（LayerNorm，LN），Baevski与Auli（2019）在每个块之后应用残差连接。

MLP 包含两个层和一个 GELU 非线性激活函数。
$$
\begin{align}
&\mathbf{z}_0=[\mathbf{x}_{\text{calss}};
\mathbf{x}^1_p\mathbf{E};
\mathbf{x}^2_p\mathbf{E};\cdots;
\mathbf{x}^N_p\mathbf{E}]
+\mathbf{E}_{pos},
&&\mathbf{E}\in\mathbb{R}^{(P^2\cdot C)\times D},
\mathbf{E}_{pos}\in\mathbb{R}^{(N+1)\times D}\\
&\mathbf{z}'_{l}=\text{MSA(LN(}\mathbf{z}_{l-1}))+\mathbf{z}_{l-1},
&&l=1\ldots L\\
&\mathbf{z}'_{l}=\text{MLP(LN(}\mathbf{z}'_{l}))+\mathbf{z}'_l,
&&l=1\ldots L\\
&\mathbf{y}=\text{LN}(\mathbf{z}^0_L)
\end{align}
$$

### 归纳偏差

我们注意到 ViT 比 CNN 具有更少的图像特定的归纳偏差。在 CNN 中，局部性、二维邻域结构和平移等变性被嵌入到整个模型的每一层中。在 ViT 中，只有 MLP 层是局部的和平移等变的，而自注意力层是全局的。二维邻域结构的使用数量不多：在模型的开始阶段通过将图像切割成图像块来应用二维邻域结构，而在微调阶段则通过调整不同分辨率图像的位置嵌入（如下所述）来应用二维邻域结构。除此之外，初始化时的位置嵌入不包含图像块的二维位置信息，图像块之间的所有空间关系都必须通过学习得到。

### 混合结构

作为原始图像块的替代方案，输入序列可以由 CNN 的特征图构成（LeCun等，1989）。在这种混合模型中，图像块的嵌入投影（等式(1)中的 $\mathbf{E}$ ）应用的是从 CNN 的特征图中提取的图像块。作为一种特定案例，图像块的空间大小定义为 $1\times1$，这意味着输入序列是通过简单地将特征图的空间维度摊平，然后投影到 Transformer 维度来获得的。分类时添加的输入嵌入和位置嵌入的方式如上述。

## 3.2. 微调和高分辨率

通常，我们先在大型数据集上预训练 ViT，然后在（较小的）下游任务进行微调。微调时，我们去掉了预先训练好的预测头，并附加一个初始化为零的 $D×K$ 前馈层，其中 $K$ 是下游类的数量。使用比预训练更高的分辨率进行微调往往是有益的（Touvron等，2019；Kolesnikov等，2020）。当输入更高分辨率的图像时，我们保持图像块大小不变，从而得到的有效序列长度更大。ViT 可以处理任意长度的序列（只受到内存限制），然而，预训练的位置嵌入可能不再有意义。因此，我们根据原始图像中的位置执行预训练位置嵌入的 2D 插值。请注意，这种分辨率调整和图像块提取是仅有地将图像 2D 结构的归纳偏差人工注入到 ViT 中的操作。

# Ch04 实验

我们评估了 ViT、HiT 和 ResNet 的表示学习能力。为了理解每个模型的数据需求，我们对不同大小的数据集进行了预训练，并评估了许多基准测试任务。当考虑到预训练模型的计算成本时，ViT 表现得非常好，以较低的预训练成本在大多数识别基准上达到了最先进的水平。最后，我们进行了一个使用自监督的小实验，并证明了自监督的 ViT 在未来很有前景。

## 4.1. 设置

### 数据集

为了探索模型的可伸缩性，我们使用 ILSVRC-2012 图像数据集 1k 类和 1.3M 图像（下述统一称为 ImageNet），其超集 ImageNet-21k 拥有 21k 类别和 14M 张图像（Deng等，2009），和 JFT 拥有 18k 个类别和 303M 张高分辨率图像（Sun等，2017）。我们遵循 Kolesnikov 等（2020）的规则去除了预训练数据集中关于下游任务的测试集数据。在这些数据集上训练的模型被我们迁移到几个基准任务中：ImageNet 在原始验证集标注和清理后的 ReaL 标注（Beyer等，2020），CIFAR-10/100（Krizhevsky，2009），Oxford-IIIT Pets（Parkhi等，2012）和 Oxford Flowers-102（Nilsback与Zisserman，2008）。对于这些数据集，预处理均遵循 Kolesnikov等（2020）的规则。

我们还评估了包含了 19 个任务的 VTAB 分类集（Zhai等，2019b）。VTAB 在不同的任务上评估了低数据迁移，因为其只为每个任务使用了 1000 个训练样本。这些任务被分为三组：自然任务（上述的任务，如：Pets，CIFAR等）；特定任务（医疗和卫星图像）；结构化任务（需要几何理解，如：局部化）。

### 模型变体

如表1所示，我们基于 BERT（Devlin等，2019）中的配置使用 ViT。“Base”和“Large”模型定义直接来自于 BERT，我们还添加了“Huge”表示更大的模型。在接下来的内容中，我们使用简短的符号来表示模型大小与输入图像块大小：例如，ViT-L/16 表示具有 $16×16$ 输入图像块大小的“Large”变体。请注意，Transformer 的序列长度与图像块尺寸的平方成反比，因此图像块尺寸较小的模型的计算成本更高。

![image-20231227162041941](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231227162041941.png)

表1：ViT 模型变体的细节

对于基准 CNN ，我们使用 ResNet（He等，2016），但将批归一化层（Wu与Szegedy，2015）改为组归一化（Wu与He，2018），并使用标准化后的卷积（Qiao等，2019）。这些修改改善了迁移的效果（Kolesnikov等，2020），我们将修改后的模型表示为“ResNet（BiT）”。对于 HiT，我们将中间特征图按照 $1\times1$ 的图像块大小输入到 ViT。为了实验不同的序列长度，我们要么①取常规的 ResNet50 的第 $4 $阶段的输出，要么②删除第 $4$ 阶段，但在第 $3$ 阶段中放置相同数量的层（保持层的总数不变），然后取这个扩展后模型的第 $3 $阶段的输出。选项②导致了一个 $4$ 倍长的序列长度，和一个更昂贵的 ViT 模型。

### 训练和微调

我们使用 Adam（Kingma与Ba，2015）训练了所有的模型（包括：ResNets），参数 $β_1=0.9$，$β_2=0.999$，`batch_size=4096`，和高权重衰减 `weight_decay=0.1` ，并且这个配置对所有模型的迁移都有用（附录[D.1]表明，在 ResNet 的实践中 Adam 的设置效果比 SGD 稍好)。我们使用线性学习率预热和衰减，详见附录[B.1]。在微调阶段，我们对于所有模型都使用具有动量的 SGD，其中 `batch_size=512`，详见附录[B.1.1]。对于表2中的 ImageNet 结果，我们以更高的分辨率进行了微调：ViT-L/16 为 512，ViT-H/14 为 518，我们还使用了Polyak与Juditsky（1992），平均的因子为 $0.9999$（Ramachandran等，2019；Wang等，2020b）。

### 测度

我们通过少量快照或微调精度来实现下游任务数据集的结果报告。在不同的数据集上微调后，微调精度捕捉了每个模型的性能。少量快照精度则是通过求解一个正则化的最小二乘回归问题来取得的，这个问题将训练图像子集的（冻结）表示映射到 $\{-1,1\}^K$ 的目标向量。这个公式使得我们能够恢复封闭形式的精确解。虽然，我们主要关注的是微调的性能，但有时也会使用线性的少量快照精度来实现快速的即时评估，不用微调精度是因为其成本太高。

## 4.2. 与最新工作比较

我们首先将我们最大的模型（ViT-H/14、ViT-L/16）与文献中最先进的 CNN 进行了比较。第一个比较点是 Big Transfer（BiT）（Kolesnikov等，2020），它在大型的 ResNets 上执行有监督的迁移学习。第二个是 Noisy Student（Xie等，2020），这是一个大型的 EfficientNet，训练是去掉标签后的 ImageNet 和 JFT- 300M 上的半监督学习。目前，Noisy Student 在 ImageNet 上是最优模型，BiT-L 在其他数据集上效果最好。所有模型都在 TPU v3 硬件上进行训练，我们报告了对每个模型进行预训练所需的 TPU v3 核$\cdot$天数，即用于训练的TPU v3核数（每个芯片2个）乘以训练时间（多少天）。

表2所示，在 JFT-300M 上预训练的较小的 ViT-L/16 模型在所有任务上都优于 BiT-L（在同一数据集上预训练），但其训练所需的计算资源要少得多。更大的模型 ViT-H/14 进一步提高了性能，特别是在更具挑战性的数据集：ImageNet、CIFAR-100、VTAB。有趣的是，该模型仍然比之前的模型所用的计算资源要少得多。然而，我们注意到，预训练的效率不仅会受到架构选择的影响，还会受到其他参数的影响，如：训练计划、优化器、权重衰减等。我们在[第4.4节]中提供了对不同架构的性能与计算的受控研究。最后，在 ImageNet-21k 公共数据集上预训练的 ViT-L/16 模型在大多数数据集上也表现良好，同时花费更少的资源：它可以在大约 30 天内使用 8 核的标准云 TPU v3 进行训练。

![image-20231227164658131](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231227164658131.png)

表2：在流行的图像分类基准中，我们与最先进的模型进行了比较。报告有精度的均值和标准差，每个模型都平均执行超过三次的微调。在 JFT-300M 数据集上预训练的 ViT 模型在所有数据集上都优于基于 ResNet 的基准，同时其预训练需要的计算资源要少得多。在较小的 ImageNet-21k 公共数据集上预训练的 ViT 也表现良好。$^*$比 Touvron等（2020）报告的 $88.5\%$ 结果略好。

在图2中 VTAB 任务分解为独立的组，并在此基准上与之前的最先进的方法进行了比较：BiT，VIVI（在 ImageNet 和 Youtube（Tschannen等，2020）上联合训练的 ResNet），以及 S4L（在 ImageNet 上的有监督加半监督学习（Zhai等，2019a））。ViT-H/14 在 *自然任务* 和 *结构化任务* 上均优于 BiT-R152x4 和其他方法。在 *特定任务* 上，前两种模型的性能相似。

![image-20231227165649146](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231227165649146.png)

图2：在自然任务、特定任务和结构化任务组中 VTAB 的性能分解。

## 4.3. 预训练的数据需求

ViT 在一个大型的 JFT-300M 数据集上进行预训练时表现良好。想比 ResNets 对视觉的归纳偏差更少，数据集的大小有多重要？我们进行了两个系列的实验。

![image-20231228163439743](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228163439743.png)

表5：在 ImageNet、ImageNet-21k、JFT300M 上进行预训练时，ViT 在不同数据集上的最高精度（%）。这些值对应于正文中的图3。模型微调分辨率为 $384$。请注意，ImageNet 的结果计算没有使用其他技术（Polyak平均和分辨率 $512$ 的图像）来实现表2中的结果。

首先，我们在规模不断增大的数据集（ ImageNet、ImageNet-21k、JFT- 300M）上对 ViT 模型进行了预训练。为了提高在较小的数据集上的性能，我们优化了三个基本的正则化参数：权值衰减（Weight Decay）、暂退率（DropOut）和标签平滑（Label Smoothing）。图3显示了对 ImageNet 进行微调后的结果（在其他数据集上的结果如表5所示[^注2]）。当在最小的数据集 ImageNet 上进行预训练时，ViT-Large 的表现不如 ViT-Base 模型，即使存在（中等）正则化。通过 ImageNet-21k 的预训练，他们的表现也很相似。只有使用 JFT-300M，我们才能看到更大模型的全部好处。图3还显示了由不同大小的 BiT 模型所跨越的性能区域。BiT CNN 在 ImageNet 上的表现优于 ViT，但随着更大的数据集，ViT 则表现更好。

![image-20231227173814454](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231227173814454.png)

图3：迁移到 ImageNet。 在小数据集上预训练时，大型的 ViT 模型性能不如 BiT ResNets（阴影区域）；而在大数据集上则刚好相反。同样地，大型的 ViT 变体也随着数据集尺度的增长从而超越了小型的 ViT 变体的性能。

[^注2]: 注意，ImageNet 预训练模型在ImageNet上进行了微调。因为在微调期间，提高分辨率也会提高性能。

其次，我们在 9M、30M 和 90M 的随机子集以及完整的 JFT- 300M 数据集上训练我们的模型。我们不对较小的子集执行额外的正则化，而是对所有设置使用相同的超参数。通过这种方式，我们评估了模型的内在性质，而不是正则化的影响。然而，我们确实使用了早期停止技术（early-stopping），并报告了在训练期间获得的最佳验证精度。为了节省计算量，我们报告了少量快照的线性精度，而不是完全的微调精度，图4包含了这些结果。在较小的数据集上，同等计算量下 ViT 比 ResNets 过拟合更多。例如，ViT-B/32 比 ResNet50 略快；它在 9M 子集上表现得更差，但在 90M+ 子集上表现得更好。ResNet152x2 和 ViT-L/16 也是如此。这一结果强化了一种直觉，即卷积的归纳偏差对于较小的数据集是有用的，但对于较大的数据集，直接从数据中学习相关模式就足够了，甚至是有益的。

![image-20231227174125894](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231227174125894.png)

图4：在 ImageNet 上线性的少量快照关于预训练尺寸的评估。在小型预训练数据集上，ResNets 相比 ViT 的性能更优，但是很快就达到上限，而 ViT 在大型预训练数据集上表现更好。ViT-b 是将 ViT-B 的所有隐藏维度都减半。

总的来说，ImageNet 上的少量快照结果（图4），以及 VTAB 上的低数据结果（表2）表明 ViT 似乎很适合非常低的低数据迁移。进一步分析 ViT 的少量快照特性是未来工作的一个令人兴奋的方向。

## 4.4 尺度研究

通过评估来自 JFT-300M 的迁移性能，我们对不同的模型进行了受控的尺度研究。在这种设置下，数据大小不是模型性能的瓶颈，我们评估了每个模型的性能与预训练的成本的关系。模型集包括：7 个 ResNets，其中 R50x1，R50x2，R101x1，R152x1，R152x2 预训练 7 个迭代，其他 R152x2、R200x3 预训练 14 个迭代；6 个 ViT，其中 ViT-B/32、ViT-B/16、ViT-L/32、ViT-L/16 预训练 7 个迭代，其他 ViT-L/16、ViT-H/14 预训练 14 个迭代；5 个 HiT，其中 R50+ViT-B/32、R50+ViT-B/16、R50+ViT-L/16、R50+ViT-L/32，R50+ViT-L/16 预训练 7 个迭代，其他 R50+ViT-L/16 预训练 14 个迭代（对于 HiT，模型名称末端的数字不代表图像块大小，而是 ResNet 骨干网的总降采样比例）。

![image-20231227180101600](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231227180101600.png)

图5：不同架构的预训练计算代价与性能的关系：ViT、HiT 和 ResNets。在相同的计算条件下，ViT 通常优于 ResNets。HiT 在较小的模型尺寸上改进了纯 Transformer ，但对于较大的模型尺寸，差距消失了。

在图5中包含了迁移性能与总的预训练计算代价的关系（关于计算成本的详细信息见附录[D.5]）。每个模型的详细结果见表6，并且在表中还可以观察到一些模式：① ViT 相比 ResNets 在性能/计算的平衡上占据了主要优势。ViT 使用了大约 $1/2$ 到 $1/4$ 的计算资源就能获得相同的性能（超过5个数据集上的平均）；②混合模型在较小的计算代价下的性能略优于 ViT，但在较大的模型中，差异消失了。这个结果有些令人惊讶，因为人们可能会期望卷积局部特征处理来帮助任何大小的 ViT；③ ViT 似乎没有在试验范围内饱和，激励了未来的扩展努力。

![image-20231228091347660](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228091347660.png)

表6：不同尺度模型实验结果的细节。这些都与图5相对应。我们展示了在几个数据集上的迁移精度，以及预训练的计算代价（exaFLOPs）。

## 4.5. 观察ViT

为了从头理解 ViT 如何处理图像数据，我们分析了它的内部表示。ViT 的第一层线性地将摊平的图像块投影到一个低维空间，详见等式(1)。在图7（左）显示了学习到的嵌入滤波器的顶层主成分。这些成分类似于有道理的基函数，这些基函数实现了每个图像块内部精细结构的低维表示。

![image-20231228095714063](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228095714063.png)

图7：（左）ViT-L/32 的 RGB 值的最初的线性嵌入滤波器。（中）ViT-L/32 的位置嵌入的相似性。依据指示的行与列，展开图显示了图像块的位置嵌入与其他图像块的位置嵌入之间的余弦相似性。（右）按注意力头和网络深度区分的注意力区域的大小。每个点展示了一层中16个头中的1个头在图像上的平均注意力距离。详见[D.7]

图像块投影到低维空间后，学习到的位置嵌入添加到图像块的表示中。从图7（中）可知，模型通过位置嵌入的相似性来学习如何对图像内的距离进行编码，即越近的图像块往往具有更相似的位置嵌入。此外，还会出现行-列结构；同一行/列中的图像块具有类似的嵌入。最后，对于更大的栅格，正弦结构有时更直观（附录[D]）。位置嵌入已经学习了二维图像拓扑关系的表示，这也就是手工制作的 2D-感知的嵌入学习的变体没有带来性能上的改进的原因（附录[D.4]）。

自注意力允许 ViT 整合整个图像的信息，即使是在最下面的层。我们研究了网络在多大程度上利用了这种能力。具体来说，在信息被整合过的图像空间中，我们根据注意力权重计算了平均距离，详见图7（右）。这种“注意力距离”类似于 CNN 中的接受野大小。我们发现，一些注意力头已经在网络最下面的层关注了图像的大部分内容，这表明模型确实使用了全局集成信息的能力。而在网络最下面的层中其他的注意力头的注意力距离则一直很小。这种高度局部化的注意力在混合模型（在 Transformer 之前应用 ResNet）中则没有那么明显，详见图7（右）。这种情况表明混合模型可能与 CNN 中的早期卷积层具有相似的功能。此外，注意力距离还随着网络深度的增加而增加。在全局范围内，我们发现模型关注的是与分类有语义相关性的图像区域（图6）

![image-20231228110345955](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228110345955.png)

图6：从输出标记到输入空间的注意力表示案例，详见[D.7]。

## 4.6. 自监督

Transformer 在 NLP 任务上具有令人印象深刻的表现。然而，他们的成功不仅来自于他们优秀的可伸缩性，而且还受益于大规模的自监督的预训练(Devlin等, 2019; Radford等, 2018)。我们还模仿了 BERT 中使用的掩膜语言建模任务，对自监督的 *掩膜图像块预测* 进行了初步的探索。通过自监督预训练，我们较小的 ViT-B/16 模型在ImageNet 上达到了 $79.9\%$ 的准确率，比从头开始训练显著提高了 $2\%$，但仍比有监督预训练低 $4\%$。附录[B.1.2]包含了更多的细节。我们将对比预训练(Chen等，2020b；He等，2020；Bachman等，2019；Henaff等，2020）的探索留给未来的工作。

# Ch05 结论

我们探索了 Transformer 在图像识别中的直接应用。与之前在计算机视觉中使用自注意力的工作不同，除了最初的图像块提取步骤外，我们没有在架构中引入特定于图像的归纳偏差。相反，我们将图像解释为图像块序列，并使用 NLP 中使用的标准 Transformer 编码器对其进行处理。在结合大型数据集上的预训练时，这种简单但可扩展的策略工作得极好。因此，ViT 基于相对较低的预训练代价，仍然在许多图像分类数据集上匹配或超过了最先进的工作。

虽然这些最初的成果令人鼓舞，但研究仍存在许多挑战。一个挑战是将 ViT 应用于其他计算机视觉任务，如检测和分割。我们的研究结果，加上 Carion 等（2020）的研究结果，表明了这种方法的前景。另一个挑战是继续探索自监督的预训练方法。我们最初的实验表明，自监督预训练有所改进，但它与大规模数据集的有监督预训练之间仍存在很大的差距。最后，进一步扩展 ViT 从而尽可能地提升性能。

# A. 多头自注意力

标准的 $\mathbf{qkv}$ 自注意力（Self-Attention, SA）[^Vaswani等 (2017)]是一种流行的用于神经网络架构的构建块。对于输入序列 $\mathbf{z}\in\mathbb{R}^{N\times D}$ 中的每个元素，我们计算了序列中所有值 $\mathbf{v}$ 的加权和。注意力权重 $A_{ij}$ 是基于序列的两个元素与其各自的查询表示 $\mathbf{q}^i$ 和键表示 $\mathbf{k}^j$ 之间的成对相似性。
$$
\begin{align}
[\mathbf{q,k,v}]&=\mathbf{zU}_{qkv},&\mathbf{U}_{qkv}&\in\mathbb{R}^{D\times3D_h}\\
A&=\text{softmax}(\mathbf{qk}^\top/\sqrt{D_h}),&A&\in\mathbb{R}^{N\times N}\\
\text{SA}(\mathbf{z})&=A\mathbf{v}.
\end{align}
$$
多头自注意力（Multihead Self-Attention, MSA）是 SA 的扩展，在 MSA 中并行地运行 $k$ 个自注意力操作，称之为“头”，然后再将它们拼接后的输出执行投影。为了保证计算代价和参数数量不变，当 $k$ 变化时，$D_h$ 也跟着变化，$D_h=D/k$，详见等式(5)。
$$
\begin{equation}
\text{MSA}(\mathbf{z})=[\text{SA}_1(z);\text{SA}_2(z);\cdots;\text{SA}_k(z)]\mathbf{U}_{msa},
\quad\mathbf{U}_{msa}\in\mathbb{R}^{k\cdot D_h\times D}
\end{equation}
$$

# B. 实验细节

## B.1. 训练

在表3中总结了针对不同模型的训练设置。我们发现，在 ImageNet 上从头开始训练模型时，强正则化是关键。在训练过程中，Dropout 应用于除 qkv-投影之外的每一个稠密层之后，并直接将位置编码添加到图像块的嵌入之后。训练 HiT 时使用的精确设置与它们对应的 ViT。最后，所有的训练都是基于分辨率 $224$ 来完成的。

![image-20231228150218796](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228150218796.png)

表3：用于训练的超参数。所有模型的训练参数：`batch_size=4096`、`learning_rate_warmup=10k`。对于 ImageNet，我们发现在全局1-范数上额外应用梯度裁剪是有好处的。训练的分辨率均为 $224$。

### B.1.1. 微调

我们使用 SGD（`momentum=0.9`）对所有的 ViT 模型进行了微调。我们基于学习率运行了一个小的网格搜索，学习率的范围见表4。为此，我们对训练集使用小的子分割（Pets and Flowers 为 $10\%$，CIFAR 为 $2\%$，ImageNet 为 $1\%$）作为开发集，并将分割后剩余的数据用于训练。对于最终的结果，我们在整个训练集上进行训练，并基于各自的测试数据进行评估。对于 ResNets 模型和 HiT 模型的微调，我们使用完全相同的设置，除了 ImageNet 的学习率扫描覆盖中添加了另一个值 $0.06$。另外，对于 ResNets，我们还运行了 Kolesnikov等（2020）的设置，并基于这次运行和我们的运行扫描覆盖中选择了最佳的结果。最后，如果没有特别提及，所有的微调实验都以分辨率 $384$ 运行（与训练阶段不同的分辨率运行微调是常见的做法[^Kolesnikov等，2020]）。

![image-20231228145934851](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228145934851.png)

表4：微调的超参数。所有模型都使用余弦学习率衰减、`batch_size=512`、无权重衰减和全局1-范数下的梯度裁剪进行微调。如果没有另外提到，微调分辨率均为 $384$。

当将 ViT 模型迁移到另一个数据集时，我们移除了整个头部（两个线性层），并替换为一个初始化为零的线性层，输出目标数据集所需的类别数。我们发现这比简单地重新初始化最后一层更稳健一些。

对于 VTAB 数据集，我们遵循 Kolesnikov等（2020）中的协议，并对所有任务使用相同的超参数设置。我们使用 $0.01$ 的学习率，并训练了 $2500$ 步，详见表4。通过对两个学习率和两个时间表运行一个小的扫描覆盖，并选择在验证集（200个样本）上 VTAB 分数最高的设置。我们遵循 Kolesnikov等（2020）中使用的预处理，除了我们不使用特定于任务的输入分辨率。相反，我们发现对于所有的任务运行在高分辨率（$384\times384$）的 ViT 收益最大。

### B.1.2. 自监督

我们为初始的自监督实验设定了 *掩码图像块预测* 目标。为了做到这一点，我们破坏了 $50\%$ 的图像块嵌入，方法是通过用可学习的 `[mask]` 标记嵌入（80%）、随机的其他图像块嵌入（10%）或保持不变（10%）来替换待破坏的图像块嵌入。该设置与 Devlin等（2019）为语言学习选择的设置非常相似。最后，我们使用它们各自的图像块表示来预测每个损坏的图像块的3位平均颜色（即总共512种颜色）。

我们在 JFT 数据集上，批大小为 4096 条件下，执行了 1M 步（大约 14 个迭代）训练自监督模型。我们使用Adam，其基本学习率为 $2·10^{−4}$，预热步长为 10k 步，余弦学习率衰减。作为预测目标预训练，我们尝试以下设置：①预测平均3位颜色（即在 512 种颜色中预测 1 个结果），②并行预测 $16×16$ 缩小版的 $4×4$ 图像块的3位颜色（即在 512种颜色中预测 16 个结果），③在整个图像块使用 L2 做回归（即在 3 个 RGB 通道上完成 256 个回归）。令人惊讶的是，我们发现一切结果都很好，尽管 L2 稍微差一些。我们只报告选项①的最终结果，因为它展示了最好的少量快照的表现。我们还实验了 Devlin等（2019）使用的 $15\%$ 的破坏率，但在我们的少量快照指标上，结果也略差。

最后，我们想指出的是，掩码图像块预测的实例化想在 ImageNet 分类上获得类似的性能提高，并不需要如此大量的预训练，也不需要像 JFT 这样的大型数据集。也就是说，我们观察到在 100k 的预训练步骤后，下游性能的收益在下降，并且在 ImageNet 上进行预训练时也看到了类似的情况。

# C. 附加结果

我们报告了与本文中所提供的数字相对应的详细结果。表5对应于论文中的图3，展示了不同的 ViT 模型在规模不断增大的数据集上（ImageNet、ImageNet-21k、JFT-300M）预训练后的迁移性能。表6对应于论文中的图5，显示了不同大小的 ViT、HiT 和 ResNet 的迁移性能，以及估计的预训练计算成本。

# D. 附加分析

## D.1. ResNets 中的 SGD 与 ADAM

ResNets 通常用 SGD 进行训练，而我们使用 Adam 作为优化器是与众不同的。在这里我们展示了导致这种选择的实验。也就是说，我们比较了两种 ResNets（50x1 和 152x2）的微调性能，这两种网络都在 JFT 数据集上使用 SGD 和 Adam 进行了预训练。对于 SGD，我们使用的是 Kolesnikov等（2020）推荐的超参数，结果见表7。在大多数数据集上和平均性能上 Adam 预训练优于 SGD 预训练。这也是在 JFT 上预训练 ResNets 时使用 Adam 作为优化器的原因。请注意，这个绝对数字比 Kolesnikov等（2020）中报告的要小，因为我们只训练了 7 个迭代，而不是 30 个迭代。

![image-20231228163805277](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228163805277.png)

表7：微调 ResNet 模型，该模型使用 Adam 和 SGD 实现预训练。

## D.2. Transformer 的形状

我们对 Transformer 架构的不同维度扩展进行了消融实验，以找出非常大的模型的最适合尺度。图8显示了不同配置在ImageNet 上的 5 个快照性能。所有配置都基于 8 层的 ViT 模型，$D = 1024$，$D_{MLP} = 2048$和图像块大小为 $32$，这个配置是图8中所有线的交点。我们还可以发现，在 $64$ 层之前清晰可见缩放的深度会产生最大的改进。然而，在 $16$ 层之后，收益已经开始下降。有趣的是，缩放网络的宽度似乎会导致最小的变化。在没有引入参数的情况下，减小图像块大小从而增加有效序列长度显示出惊人的稳健的改进。这些发现表明，计算的付出可能比参数的数量更好地预测性能，即比例缩放应该强调深度而不是宽度。总的来说，我们发现按比例缩放所有维度都会导致稳健的改进。

![image-20231228163954266](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228163954266.png)

图8：缩放不同的 ViT 模型维度。

## D.3. 头的类型和 `CLASS` 标记

为了尽可能接近原始的 Transformer 模型，我们使用了一个额外的 `[CLASS]` 标记作为图像表示。然后，该标记的输出通过一个小的多层感知器（MLP）转换为类预测，该 MLP 以 $\tanh$ 作为单隐层的非线性激活函数。

这个设计继承自文本的 Transformer 模型，我们在整个主论文中使用它。最初尝试只使用图像块嵌入与全局平均池（Global Average Pooling，GAP），然后是一个线性分类器（就像 ResNet 的最终特征图一样）表现得非常差。然而，我们发现这个结果既不是额外的标记造成的，也不是受 GAP 操作的影响。反而，对不同学习率的需求却完全地解释了性能上差异的原因，详见图9。

![image-20231228165337745](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228165337745.png)

图9：类标记和全局平均池化分类器的比较。两者的效果相似，但需要不同的学习率。

## D.4. 位置嵌入

我们的消融实验是使用不同的位置嵌入来编码空间信息：

- 不提供位置信息：输入看作 *图像块袋*（类似于词袋）；
- 1D 位置嵌入：输入看作栅格顺序中的图像块序列（本文所有其他实验的默认配置）
- 2D 位置嵌入：输入看作二维的图像块栅格。在这种情况下，学习两组嵌入，每组用于一个轴（X-轴嵌入和Y-轴嵌入），每组的大小都是 $D/2$。然后，根据输入路径上的坐标，我们将两组嵌入拼接起来获得图像块的最终嵌入。
- 相关位置嵌入：使用图像块之间的相对距离而不是绝对位置来编码空间信息。为此，我们使用 1D **相对注意力**，在其中我们定义了所有可能的成对图像块的相对距离。因此，对于每一个给定的图像块对（一个作为查询，另一个作为注意力机制中的键/值），我们都有一个偏移量$p_q−p_k$，其中每个偏移量都与一个嵌入相关联。然后，我们只需简单地运行额外的注意力模块，其中使用原始查询（即查询的内容），但使用相对位置嵌入作为键。然后，我们使用来自于相对注意力的 logits 作为偏差项，并在应用 softmax 之前将其与主注意力模型（基于内容的注意力模块）的 logits 相加。

除了空间信息的不同编码方式外，我们还尝试将这些信息通过不同的方法合并到我们的模型中。对于 1D 和 2D 的位置嵌入，我们尝试了三种不同的情况： (1)在输入通过模型主干之后、进入到 Transformer 编码器之前，将位置嵌入添加进去（本文中所有其他实验的默认值）；(2)在每一层输入的开始处添加可学习的位置嵌入；(3)在每一层输入的开始处添加学习到的位置嵌入（层之间共享）。

![image-20231228171500276](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228171500276.png)

表8：用 ImageNet 5个快照线性化评价 ViT-B/16 模型对位置嵌入的消融研究结果。

在表8中总结了对 ViT-B/16 模型的消融研究的结果。由此可见，虽然无位置嵌入的模型与有位置嵌入的模型之间的性能有很大的差距，但不同的位置信息编码方式之间几乎没有差别。我们推测，由于 Transformer 编码器操作的输入是图像块级而不是原始像素级，因此如何编码空间信息的差异不是那么重要的。更准确地说，在图像块级输入中，空间维度比原始像素级输入要小得多，例如，$14×14$ 与 $224×224$，并且对于这些不同的位置编码策略，在这个低分辨率下同样很容易学习到空间关系的表示。即便如此，网络学习到的特定模式的位置嵌入的相似度还是取决于训练的超参数（图10）。

![image-20231228171655075](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228171655075.png)

图10：使用不同超参数训练模型的位置嵌入。

## D.5. 实证的计算成本

由于受到通道宽度和缓存大小等细节影响，理论上的 FLOPs 并不能稳定地预测模型在我们的硬件架构上的真实速度。为了解决这个问题，我们在 TPU v3 加速器上对感兴趣的主要模型执行推理时的速度进行计时；推理和反向传播速度之间的差异是一个不变的独立于模型的因素。

图12（左）展示了一个核心在不同的输入尺寸下每秒可以处理多少张图像。每个点指的是在大范围的批大小内测量到的峰值性能。可以看出，ViT 关于图像尺寸在理论上的双二次方缩放，对于最大分辨率下的最大模型仅仅才开始。

另一个感兴趣的量是将每个模型装入到一个核心的最大批大小，越大越适合扩展到大型数据集。图12（右）显示了同一组模型的这个量值，结果表明，大型 ViT 模型在内存效率方面比 ResNet 模型具有明显的优势。

![image-20231228173835415](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231228173835415.png)

图12：（左）跨越输入尺寸大小的各种架构的真实时间，ViT 模型的速度堪比类似的 ResNets。（右）跨越输入尺寸大小的各种架构装入到设备中一个核的最大批大小，ViT 模型的内存效率显然更高。

## D.6. 轴向注意力

轴向注意力（Huang等，2020；Ho等，2019）是一种简单而有效的技术，可以在被组织为多维张量的大型输入上运行自注意力。轴向注意力的一般思想是执行多个注意力操作，每个操作沿着输入张量的一个轴，而不是对摊平的输入应用一维注意力。在轴向注意力中，每个注意力沿着一个特定的轴混合信息，同时保持沿着其他轴的信息独立。沿着这条线，Wang等（2020b）提出了轴向 ResNet 模型，在该模型中，ResNet50 中所有核大小为 $3×3$ 的卷积都被轴向自注意力取代，即行和列注意力，并通过相对位置编码增强。我们已经实现了AxialResNet 作为一个基准模型[^注3]。

[^注3]: 我们是基于[开源的 PyTorch 项目](https://github.com/csrhddlam/axial-deeplab)来实现的。在我们的实验中，我们在准确性方面复制了（Wang等，2020b）中报告的分数，然而，因为我们的实现类似于开源实现，因此在 TPU 上非常缓慢。所以，我们无法将它用于广泛的大规模实验。这些问题都可以通过一个精心优化的实现来解锁。

此外，我们修改了 ViT ，使用处理 2D 形状输入的模块代替处理图像块的 1D 序列的模块，还使用轴向 Transformer 块（行自注意力+MLP→列自注意力+MLP）来代替以往的 ViT 中的模块（自注意力+MLP）

图13中对比了 AxialResNet、Axial-ViT-B/32 和 Axial-ViT-B/16 在 ImageNet 5个快照线性器上的性能，以及在 JFT 数据集上预训练计算时 FLOPs 的数量和推理时间（每秒样本数）。正如我们所见，Axial-ViT-B/32 和 Axial-ViT-B/16 都比 ViT-B 中它们所对应的模型的性能要好，但这个优势是以更多计算为代价的。这是因为在 Axial-ViT 模型中，每个具有全局自注意力的 Transformer 块被两个 Axial Transformer 块（一个用行自注意力、一个用列自注意力）所替代，虽然在轴向（axial）案例中自注意力的序列长度较小，但是每个 Axial-ViT 块中都包含一个额外的 MLP。尽管 AxialResNet 在准确性/计算权衡方面看起来是合理的（图13，左），但在 TPU 上，简单实现的性能非常慢（图13，右）。

![image-20231229104328564](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231229104328564.png)

图13：基于轴向注意力的模型的性能。在 ImageNet 5个快照线性器的最佳精度与对应速度的比较： FLOPs 数量（左）和推理时间（右）。

## D.7. 注意力距离

为了理解 ViT 如何利用自注意力来整合整个图像中的信息，我们在不同网络层上分析了注意力权重所跨越的平均距离（图11），这种“注意力距离”类似于 CNN 中的感受野大小。平均注意力距离在较低网络层的注意力头中发生了高度的变化，一些注意力头关注大部分图像，而另一些则关注查询位置当前或附近的小区域。随着深度的增加，所有注意力头的注意力距离都会增加。在网络的第二部分，大多数注意力头都跨越了标记实现广泛地关注。

![image-20240104114230627](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20240104114230627.png)

图11：按照注意力头和网络深度区分的关注区域尺寸。注意力距离是由128样本图像计算得到的，计算过程是对查询像素和其他像素之间的距离依据注意力权重求平均。每个点显示了一层中16个注意力头中的一个在图像之间的平均注意力距离。图像宽度是 224 个像素。

## D.8. 注意力图

为了计算从输出标记到输入空间的注意力图（图6和图14），我们使用了 Attention Rollout（Abnar与Zuidema，2020）。简单地说，我们平均了 ViT-L/16 的所有注意力头的注意力权重，然后递归地乘以所有层的权重矩阵。这就解释了注意力在所有层中跨标记的混合。

![image-20231229110903966](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231229110903966.png)

图14：进一步的样本注意力图，如图6（随机选择）。

## D.9. ObjectNet 的结果

我们还根据 Kolesnikov等（2020）的评估设置，在 ObjectNet 基准上评估了我们的旗舰模型 ViT-H/14，结果是得到了 $82.1\%$ 的前 5 精度和 $61.7\%$ 的前 1 精度。

## D.10. VTAB 细目分类

表9展示了每个 VTAB-1k 任务上模型的得分。

表9：跨越任务的 VTAB-1k 的性能的细目分类

![image-20231229111136316](images/深度学习/通用模型/Transformer/二维图像/ViT%20Transformers%20for%20Image%20Recognition%20at%20Scale/image-20231229111136316.png)

# 参考文献

[对标 GLUE、ImageNet，谷歌推出视觉任务适应性基准 VTAB - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/90823589)