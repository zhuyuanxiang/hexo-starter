---
title: 通过核相关和图池化来挖掘点云的局部结构
excerpt: 与图像不同，在深度网络上实现三维点云的语义学习具有挑战性。在现有的工作中，PointNet通过直接的点集学习取得了很好的效果。然而，它并没有充分利用一个点的局部邻域，由于在这个邻域中包含了细粒度结构信息，从而有助于实现更好的语义学习。在这方面，我们提出了两种新的操作来改进PointNet，以更有效地利用点的局部结构。①侧重于局部的三维几何结构。类似于图像的卷积核，我们定义了“点集核”为一组可以学习的三维点。这些三维点根据核相关性度量了几何的仿射性质，从而共同地响应一组相邻的数据点，这个方法参考了点云配准中的类似技术。②利用局部高维特征结构。这个特征结构是在最近邻图上通过特征的递归聚合得到的，而最近邻图是在三维坐标上计算出的。实验表明，该网络能够在主要数据集上有效地捕获局部信息，并稳健地获得更好的性能。
categories:
  - 点云特征
tags:
  - 点云特征
  - PointNet
  - Kernel Correlation
  - Graph Pooling
  - ShapeNet
  - ModelNet
date: 2023-11-28
updated: 2023-11-28
toc: true
typora-root-url: D:\Projects\Github\zhuyuanxiang\hexo_pages\hexo-starter\source\_posts\
---

# 通过核相关和图池化来挖掘点云的局部结构

Shen Y, Feng C, Yang Y, et al. Mining point cloud local structures by kernel correlation and graph pooling[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 4548-4557.

[原始论文](https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Mining_Point_Cloud_CVPR_2018_paper.pdf)

[中文翻译]()

[原始代码_PyTorch](https://github.com/ftdlyc/KCNet_Pytorch)

论文作者实验室主页：[Software & Data Downloads | Mitsubishi Electric Research Laboratories (merl.com)](https://www.merl.com/research/license#KCNet)

# 摘要

与图像不同，在深度网络上实现三维点云的语义学习具有挑战性。在现有的工作中，PointNet通过直接的点集学习取得了很好的效果。然而，它并没有充分利用一个点的局部邻域，由于在这个邻域中包含了细粒度结构信息，从而有助于实现更好的语义学习。在这方面，我们提出了两种新的操作来改进PointNet，以更有效地利用点的局部结构。①侧重于局部的三维几何结构。类似于图像的卷积核，我们定义了“点集核”为一组可以学习的三维点。这些三维点根据核相关性度量了几何的仿射性质，从而共同地响应一组相邻的数据点，这个方法参考了点云配准中的类似技术。②利用局部高维特征结构。这个特征结构是在最近邻图上通过特征的递归聚合得到的，而最近邻图是在三维坐标上计算出的。实验表明，该网络能够在主要数据集上有效地捕获局部信息，并稳健地获得更好的性能。

# Ch01 简介

随着各种三维传感器的快速发展，三维数据变得无处不在，由于[^3,20,29,32,41]在机器人技术、自动驾驶、逆向工程和民用基础设施监测方面的广泛应用，利用深度网络对这类数据的语义理解和分析正受到人们的关注。特别是，三维点云作为最原始的3D数据格式之一和3D传感器最常见的原始输出，它不能像卷积网络的2D图像那样被深度网络简单地使用。这主要是由于点的不规则组织造成的，这是此类原始数据格式所固有的一个基本挑战：相比行列索引的图像，点云是一组点坐标（可能具有强度和表面法线等属性），在点之间没有明显的顺序，从深度图中计算出来的点云除外。

然而，受图像卷积网络成功的影响，许多工作都集中在3D体素上，即在学习过程之前从点云转换成的规则的3D栅格。只有这样，3D卷积网络才能学会从体素[^2,7,23,25,30,42]中提取特征。然而，为了避免棘手的计算时间复杂度和内存消耗，以及体素方法导致的量化伪影，于是这类方法仅应用在小型的空间分辨率上。除了最近使用八叉树（Octree ）[^32,41]的一些改进外，体素方法还难以学习几何结构的细节。

与卷积网络不同，一个简单并且有效的架构PointNet [^29]可以直接学习点集。首先，PointNet计算每个点的多层感知器（MLP）的单个点特征，然后聚合所有特征作为点云的全局表示。尽管PointNet在不同的三维语义学习任务上获得了最先进的结果，但是它直接将每个点特征聚合到全局特征的方式表明，其在实际操作中并没有充分利用点的局部结构来捕获细粒度模式：每个点的多层感知器（MLP）的输出仅粗糙地编码了该三维点的存在性，表明这个三维点是在三维空间中的一个确定的非线性划分中。如果MLP不仅可以编码点的存在性，还可以编码在三维空间的非线性划分中存在的该点的特征（例如：角与平面、凸性与凹性等等），则可以获得一个更有判别性的表示。本文的动机就是从三维物体表面上的点的局部邻域中学习这种特征信息。

为了解决上述问题，PointNet++ [^31]曾提出将一个点集（set）分割成更小的点簇（cluster），并且将这个小的点簇输入到小的PointNet，并对高维特征点集迭代地重复这样的过程，这导致了复杂的架构，并且降低了网络的速度。因此，我们试图从另一个的方向来探索：是否存在其他有效的、可学习的局部算子，并且还具备清晰的几何解释，为增强和改进原始的PointNet带来直接的帮助，同时仍然保持其简单的体系结构？

为了解决上面提出的问题，我们在三维点云上，基于两个经典的、有监督的特征表示学习任务，通过两种新的操作来利用局部几何信息和特征结构，从而改善了PointNet，详见图2。贡献如下：

- 我们提出了一个核相关层来利用局部几何结构，并具有清晰的几何解释（见图1和图3）。
- 我们提出了一种基于图的池化层来利用局部特征结构来增强网络的稳健性。
- 我们的KCNet利用这两种新操作有效地提高了点云语义学习性能。

![图1：可视化学习到的核的相关性。](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231204170154076.png)

图1：可视化学习到的核的相关性。为了表示点p周围的复杂的局部几何结构，我们提出了核相关性来度量两个点集之间的匹配度：p的相邻点和核点。此图显示了核点的位置和宽度作为球体的中心和半径（顶部行），以及在4个对象上的5个核的对应过滤器响应（其他行）。颜色表示每个物体的亲合力（红色：最强，白色：最弱）。注意由不同的内核捕获的各种结构（平面、边、角、凹面和凸面）。

![图2：我们的KCNet架构。](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231204170629437.png)

图2：我们的KCNet架构。通过前端核相关层探索局部几何结构，相关层计算了每个数据点的$K$个最近邻点和$L$个点集核之间的$L$个不同的匹配度，每个前端核包含了$M$个可以学习的三维点。所得到的响应与原始的三维坐标拼接起来。局部特征结构被后续的图池化层利用，从每个点的三维欧氏邻域中离线地构建的每个三维对象实例共享一个相同的图池化层。每层都使用ReLU，但是没有层归一化（BatchNormalization），DropOut用于最后一层的MLP。其他操作和基础网络架构在形状分类和零件分割方面都类似于PointNet [29]。实心箭头表示带有反向传播的正向操作，而虚线箭头表示没有反向传播。绿框是输入数据，灰色是中间变量，蓝色是网络预测。我们的网络简称为KCNet。

![图3：人工核（线性、平面、曲面）和响应的可视化](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231205201055128.png)

图3：人工核（线性、平面、曲面）和响应的可视化，与图1类似。

# Ch02 相关工作

## 2.1. 几何的局部性质

我们将首先讨论三维数据中经常使用的一些几何的局部性质，以及它们是如何引导我们通过修改核相关性使其作为一种工具，以刻画由复杂数据驱动的、潜在的、几何的局部结构。

### 表面法线

表面法线作为一种基本的曲面性质，在三维形状重建、平面提取和点集配准[^5,14,28,39,40]等许多领域得到了广泛地应用。它们通常来自于CAD模型，也可以通过主成分分析（PCA）来进行估计，即在相邻点的数据协方差矩阵中寻找最小方差方向[^15]。在PointNet中使用每个点的表面法线[^29,31]与仅使用三维坐标的模型相比提高了性能，所建的表面法线对应于将一个点的局部邻域建模为一个平面。这个性能提升的结果符合我们之前的期望，即一个点的“类型”特征叠加位置特征应该具有更好地表示。然而，这也引出了我们的一个问题：因为法线可以从3D坐标（不像颜色或强度）来估计，那么为什么PointNet只使用三维坐标作为输入导致不能获得同样的好处？我们认为这是由于以下原因： ①每个点MLP不能仅靠3D坐标来捕获近邻信息，②全局池化操作不能或不足以实现这一目标。

### 协方差矩阵

通过数据协方差矩阵可以实现局部邻域的二阶描述，并且这类矩阵已经广泛应用于平面提取、曲率估计和法线估计等情况[^11,14]。按照来自法线估计的相同思路，局部数据协方差矩阵提供的信息实际上比法线更丰富，因为它将局部邻域建模为一个椭球体，球体之中包括秩亏缺情况下的线和平面。我们还根据经验观察到，矩阵在语义学习方面优于法线。

然而，表面法线和协方差矩阵都可以看作是人工实现的局部形状的有限描述，因为完全不同形状的点集可以共享一个相似的数据协方差矩阵。当然，为了提高细粒度类别的三维语义形状分类和三维语义分割的性能，需要对每个点的局部邻域进行更详细的分析。尽管PointNet++ [^31]是学习更有区别的描述的一种直接方法，但它可能不是最有效的解决方案。相反，我们希望找到一个可学习的局部描述，该描述简单高效，并且具有清晰的几何解释，就像上面两个手工制作的特征一样，这样它就可以直接插入到原始的、优雅的PointNet架构中。

### 核相关性

另一个被广泛使用的描述是相似性。对于图像，卷积（通常实现为互相关）可以量化输入图像和卷积核[^21]之间的相似性。然而，面对上述由点云定义的卷积带来的挑战，我们该如何测量两个点集之间的相关性？这个问题引导我们找到了核相关[^17,38]作为这样一个工具。研究表明，核相关作为成对的点距离函数是测量二维/三维点集之间几何匹配度的一种有效方法，并已用于点云配准和特征对应问题[^17,33,38]。特别是对于配准，通过迭代地精调原始点云和参考点云两者之间的刚性/非刚性转换，从而最大化它们的核相关响应，使源点云最佳匹配参考点云。因此，我们提出了核相关层，分别将局部相邻点和一个可学习的点集核作为源点云和参考点云，详见[第3.1节](#3.1. 局部几何结构上的学习)。

## 2.2 点云上的深度学习

近年来，对三维输入数据（特别是点云）的深度学习越来越受到研究关注。主要有四种方法：基于体积的方法，基于补丁的方法，基于图的方法和基于点的方法。

- **基于体积的方法**将三维空间划分为规则的体素，并在体素[^2,7,23,25,30,42]上应用三维卷积。然而，体积表示需要高内存和高计算成本才能提高空间分辨率。最近，基于八叉树（Octree）和基于KD-树的网络已经被引入，但它们仍然可能存在内存效率问题[^20,32,41]。
- **基于Patch的方法**将三维曲面参数转化为局部的Patch，并在这些Patch上应用卷积[^3,24]。这种方法的优点是曲面形变的不变性。然而，从网格推广到点云[^43]并不容易。
- **基于图（Graph）的方法**是通过图来表征点云。显然，图表示可以灵活地适应不规则数据，甚至是非欧数据，如：点云、社交网络上的用户数据和基因数据[^1,9,18,19,26,27,27]。因此，可以使用图（如：连通图或多边形网格）来表示三维点云，将图转换为谱表示，并在谱域中应用卷积[^4,8,10,13,19,22]。其他工作[^34]还研究了图中边属性的卷积，该图是建立在点云之上的，边属性来自于图中顶点的邻域。
- **基于点的方法**（如：PointNet[^29]）则直接操作于点云。为每个点学习得到空间特征，并通过最大池化操作聚合点特征获得全局特征。PointNet对于形状分类和形状分割问题是简单且有效的。然而，全局聚合没有显式地考虑局部结构，从而错过了捕获细粒度模式的机会，并且对噪声敏感。为了将PointNet扩展到局部结构学习，我们使用了一个简单的基于图的网络：构造$k$个最近邻图（K-NNG），从而为核相关利用邻域信息，然后在每个节点邻域内执行最大池化操作，于是就会发现局部点云共享着相似的几何结构。K-NNG通常用于建立局部连通信息，从而满足点云表面检测、三维目标识别、三维目标分割和压缩等应用的需要[^12,35,37]。

# Ch03 方法

我们现在通过①核相关性（度量点集的几何匹配性）和②K-NNG（在相邻点之间传播的局部特征）来解释在点的邻域上学习得到局部结构的细节，详见图2。

## 3.1. 局部几何结构上的学习

如前所述，我们从基于核相关的点云配准中获得了灵感。在网络前端，我们将一个点的局部邻域作为源，以及一组可学习的点（即：核）作为参考，刻画某些类型的局部几何结构/形状。

我们修改了原始的核相关计算方法，允许参考通过反向传播自由地调整其形状（即：核中点的坐标）。请注意，与点集配准相比，这里的视角发生了变化：我们希望通过单点的自由转换来学习模板/参考的形状，而不是使用固定的模板来寻找源点集和参考点集之间的最优转换。通过这种方式，一组可学习的核点类似于卷积核，它只激活其共同的相邻区域的点，并捕获这个以核函数及其核宽度为特征的感受野内的局部几何结构。在这种情况下，学习过程可以看作是寻找一组参考点/模板点，从而将最有效和最有用的局部几何结构进行，从而与网络中的其他参数共同获得最佳的学习性能。

具体地说，我们采用了[^38]中的留一法核相关算法（Leave-one-out Kernel Correlation, LOO-KC）[^注1]和多重链接配准代价函数的思想来捕获点云的局部几何结构。让我们将点集核$\mathcal{k}$（有$M$个可以学习的点）和点云（由$N$个点构成）中的当前锚点$x_i$用于定义核相关函数（KC）：
$$
\begin{equation}
\text{KC}(\pmb{k},\mathbf{x}_i)=
\frac1{|\mathcal{N}(i)|}\sum^M_{m=1}\sum_{n\in\mathcal{N}(i)}
K_{\sigma}(\pmb{k}_m,\mathbf{x}_n-\mathbf{x}_i)
\end{equation}
$$
其中$\pmb{k}_m$是核中的第$m$个可学习点，$\mathcal{N}(i)$是锚点$\mathbf{x}_i$的邻域索引集，$\mathbf{x}_n$是$\mathbf{x}_i$的邻域点之一。$K_{\sigma}(\cdot,\cdot):\mathbb{R}^D\times\mathbb{R}^D\rightarrow\mathbb{R}^D$是任何有效的核函数（$D = 2或3$用于2D或3D点云）。为了有效地存储点的局部邻域，我们将每个点视为一个顶点，从而预先计算出一个K-NNG，而边的连接性只与附近的顶点相关。

[^注1]: 留一法（Leave-one-out）：一种交叉验证方法，即留一个数据不参加训练，而用于测试。

根据[^38]，在不失一般性的前提下，本文选择了高斯核：
$$
\begin{equation}
K_{\sigma}(\mathbf{k,\delta})=\exp(-\frac{\|\mathbf{k-\delta}\|^2}{2\sigma^2})
\end{equation}
$$
其中，$\|\cdot\|$是两个点之间的欧氏距离；$\sigma$是核宽度，用于控制两个点之间的的距离的影响力。高斯核的一个很好的性质是，它作为两点之间距离的函数呈指数衰减，提供了从每个核点到锚点相邻点的软分配，放松了传统ICP中的不可执行微分的硬分配。我们的KC对核点和相邻数据点之间的成对点的距离进行编码，并随着两个点集的形状相似而增加编码的信息，因此它可以清楚地解释为几何相似度度量，并且在平移下保持不变。请注意在这里选择内核宽度的重要性，因为σ太大或太小会导致性能下降（见表6），与核密度估计存在的问题相同。幸运的是，对于例子中的二维或三维空间，这个参数可以通过经验选择作为所有训练点云的邻域图中的平均邻近距离。

为了完成对所提出的新的可学习层的描述，给定① $\mathcal{L}$作为网络损失函数，②其关于每个点$\mathbf{x}_i$的KC响应$d_i=\frac{\partial\mathcal{L}}{\partial\text{KC}(\mathcal{k},\mathbf{x}_i)}$从顶层开始反向传播，对于每个核点$\pmb{k}_m$的反向传播方程：
$$
\begin{equation}
\frac{\partial\mathcal{L}}{\partial\mathcal{k}_m}
=\sum^N_{i=1}\alpha_i d_i[\sum_{n\in\mathcal{N}(i)}\mathbf{v}_{m,i,n}
\exp(-\frac{\|\mathbf{v}_{m,i,n}\|^2}{2\sigma^2})]
\end{equation}
$$
其中，点$x_i$的归一化常量$\alpha_i=\frac{-1}{|\mathcal{N}(i)|\sigma^2}$；局部差分向量$\mathbf{v}_{m,i,n}=\pmb{k}_m+\mathbf{x}_i-\mathbf{x}_n$。

虽然我们的KC来源于[38]中的LOO-KC，但是操作不同： ①不同于LOO-KC作为点集与其元素点之间的紧致度量，我们的KC计算数据点邻域和可学习点核之间的相似性；②不同于多链代价函数，因为多链代价函数包含了用于固定模板变换的参数，而我们的KC允许核中的所有点自由地移动和调整（即$\pmb{κ}$没有权重衰减），从而将模板和转换参数替换为点集核。

为了更好地理解KC是如何捕获各种局部几何结构的，我们在图3中可视化了在不同对象上的手工制作了几个内核和对应的KC响应。类似地，我们在图1中可视化了从分割网络中学习到的几个内核。请注意，我们可以在KCNet中学习$L$个不同的内核，其中$L$是一个超参数，类似于卷积网络中的输出通道的数量。

## 3.2. 局部特征结构上的学习

我们的KCNet只在网络前端执行KC来提取局部几何结构，如图2所示。为了有效地存储点的局部邻域用于计算KC，我们将每个点视为一个顶点，其边只连接附近的点，建立一个K-NNG。输出的K-NNG对于开发更深层次中的局部特征结构也很有用。因为卷积网络能够局部聚合特征，并且通过多个池化层渐近地增加感受野。因此受其启发我们为了KC，使用递归的方式沿着相同的三维邻域图的边对特征传播和聚合，从而在网络的顶层利用局部特征结构。

我们的关键认知是，邻居点趋向于相似的几何结构，因此通过邻域图传播特征有助于学习更稳健的局部模式。请注意，我们特意避免了在顶层中改变这种邻域图结构，这也类似于图像上的卷积：即使输入图像的特征通道在顶层的卷积层中得到极大地扩展，每个像素的空间排序和邻域也保持不变。

具体来说，定义$\mathbf{X}\in\mathbb{R}^{N\times K}$表示图池化层的输入；$\mathbf{W}\in\mathbb{R}^{N\times N}$表示K-NNG拥有的邻接矩阵，其中顶点$i$与$j$之间存在边时$\mathbf{W}(i,j)=1$，否则为零。直觉上观察，形成局部曲面的邻居点通常共享相似的特征模式。因此，我们通过一个图池化操作来聚合其邻域内的每个点的特征：
$$
\begin{equation}
\mathbf{Y=PX}
\end{equation}
$$
这个池化操作可以是平均池化，也可以是最大池化。

**图平均池化层**通过使用等式(4)中的$\mathbf{P}\in\mathbb{R}^{N\times N}$作为归一化邻接矩阵对一个点的邻域特征进行平均，邻接矩阵的定义如下：
$$
\begin{equation}
\mathbf{P=D^{-1}W}
\end{equation}
$$
其中，$\mathbf{D}\in\mathbb{R}^{N\times N}$表示度矩阵，其第$(i,j)$项的定义如下：
$$
\begin{equation}
d_{i,j}=
\begin{cases}
\deg(i),&如果i=j\\
0,&其他
\end{cases}
\end{equation}
$$
其中，$\deg(i)$是顶点$i$的度（即与$i$相连的顶点个数）。

**图最大池化层**在每个顶点的邻域上取得最大特征，并且在$K$维的每个维度上独立操作。于是对等式(4)的矩阵乘法中的“$+$”算子替换为“$\max$”算子即可。输出$\mathbf{Y}$的第$(i,k)$个项定义为：
$$
\begin{equation}
\mathbf{Y}(i,k)=\max_{n\in\mathcal{N}(i)}\mathbf{X}(n,k)
\end{equation}
$$
其中，$\mathcal{N}(i)$表示从$\mathbf{W}$中计算得到的点$\mathbf{X}_i$的邻域索引集合。

然后，通过图的池化操作（最大池化或平均池化）得到一个点的局部特征码。该特征码可以表示局部曲面聚合后的特征信息。注意这个操作与PointNet++的关系：每个点$i$的局部邻域类似于PointNet++中的簇/段，因此这种图操作允许在原始的PointNet体系结构上进行局部特征聚合。

# Ch04 实验

现在我们讨论三维形状分类（4.1节）、部件分割（4.2节），并进行消融研究（4.3节）。

## 4.1. 形状分类

### 数据集

我们在二维和三维点云上评估了我们的网络。对于二维形状分类，我们将MNIST数据集[21]转换为二维点云。MNIST包含的是手写数字图像，其中有6万张训练图像和1万张测试图像。我们将每幅图像中的非零像素转换为二维点，保持坐标作为输入特征，并在[-0.5,0.5]内进行归一化。对于三维形状分类，我们在基准测试ModelNet10（10个类别，4899个CAD模型）和ModelNet40（40个类别，12311个CAD模型）[42]上评估了KCNet。ModelNet10被分为训练（3991个CAD模型）和测试（908个CAD模型）。ModelNet40被分为训练（9843个CAD模型）和测试（2468个CAD模型）。与PointNet一样，为了获得三维点云，我们使用MeshLab [6]的泊松圆盘采样，从网格中均匀采样到每个对象的1024个点，并将其归一化为一个单位球。

### 网络配置

如图2a所示，我们的KCNet总共有9个参数层。第一层是核相关性，它以点坐标作为输入和输出的局部几何特征，并与点坐标相拼接。然后将特征传递到前2层MLP中进行每个点的特征学习。然后，图池化层将每个点的输出特征聚合为更健壮的局部结构特征，这些特征与来自第二个2层MLP的输出拼接起来。其他配置类似于原始的PointNet，除了①在每个没有批归一化的全连接层之后使用ReLU，因为我们发现批归一化在KCNet和PointNet中没用；②对于最终的全连接层使用比例为0.5的Dropout层。我们使用16-NN图进行核计算和图最大池化。使用$L = 32$组内核，其中每个核拥有$M=16$个点，这些点在$[-0.2,0.2]$内均匀初始化，内核宽度为$σ = 0.005$。我们在NVIDIA GTX 1080 GPU上使用改进过的Caffe[16]网络训练了400个周期，其中优化器为ADAM，初始化学习率为$0.001$，批大小为$64$，矩为$0.9$，矩2为$0.999$，权重延迟为$1e-5$，无数据增强。

### 实验结果

表1和表2将我们的结果与最近的一些工作进行了比较。在MNIST数字分类中，KCNet得到了用ConvNets得到的可比性结果。在ModelNet40形状分类中，我们的方法分别比PointNet-普通版（无T-Nets）和PointNet高$3.8\%$和$1.8\%$，比PointNet++[31]略优$0.3\%$。表3总结了不同网络所需的参数数量和推理时间。注意，KCNet比[20,31]使用更少的参数获得了更好或类似的精度，计算效率更高。

![image-20231206104327045](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231206104327045.png)

表1：MNIST数字分类

![image-20231206104348154](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231206104348154.png)

表2：本文提出的方法与当今最优网络在ModelNet形状分类问题上的精度比较。我们的KCNet在ModelNet10和ModelNet40上都获得了有竞争力的性能。请注意，MVCNN[^36]和VRN聚合[^2]使用图像和体积作为输入，而其他模型均使用点云作为输入。

![image-20231206104644906](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231206104644906.png)

表3：模型大小和推理时间。“M”表示百万。网络是在拥有单个NVIDIA GTX 1080 GPU和Intel i7-9700@3.2 GHz 12 cores CPU的PC上测试的。其他配置同[^31]。

## 4.2. 部件分割

部件分割是一项重要的任务，需要精确分割具有精细结构的复杂形状。我们使用图2b所示的网络来预测3D点云对象中每个点的部件标签。

### 数据集

我们在ShapeNet的部件数据集[44]上评估了KCNet的部件分割。来自16个形状类别的3D点云对象有16,881个形状，一个对象中的每个点对应一个部件标签（总共50个部件，并且在形状类别之间不重叠）。平均每个对象包含不到6个部分，而高度不平衡的数据使这项任务相当具有挑战性。我们使用了与[第4.1节]()中相同的策略，对每个CAD对象统一采样2048个点。我们使用了与[31]相同的官方训练测试分割。

### 网络配置

分割网络有10个参数层。不同层捕获的局部特征的与复制后的全局特征及形状信息拼接起来，如[29]中所述，详见图2b。同样，ReLU在没有批归一化的每一层中使用。全连接层中使用比率为$0.3$的Dropout层。我们使用18-NN图进行核计算和图最大池化。使用了$L = 16$组内核，其中每个内核有M = 18个点，这些点在$[−0.2,0.2]$中均匀初始化，内核宽度为$σ = 0.005$。其他的超参数与形状分类中的相同。没有进行数据扩充。

### 实验结果

我们的方法与PointNet[^29]、PointNet++[^31]和Kd-Net [^20]进行了比较。使用每个类别的交并比（IoU）作为评价度量[^20]：每个形状实例的IoU是这个形状类别中每个部件出现的平均IoU（属于其他形状类别部件的IoU被忽略）。每个类别的平均IoU（mIoU）是通过将该类别中所有形状的IoU进行平均得到的。总体平均实例mIoU (Ins. mIoU)是通过平均所有形状实例的IoU来计算的。此外，我们还报告了总体平均类别mIoU (Cat. mIoU），是直接平均超过16个类别。表4列出了这些结果。与使用表面法线作为额外输入的PointNet++相比，我们的KCNet只将原始点云作为输入，并在表3中的计算和参数方面获得了更高的性能和更高的效率。图4显示了在ShapeNet部件测试数据集上的一些预测结果的示例。

![image-20231206110930037](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231206110930037.png)

表4：ShapeNet部件分割结果。基于实例的（Ins.）和基于类别的（Cat.）的平均mIoU的汇报。

![image-20231206111040257](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231206111040257.png)

图4：在ShapeNet部件测试数据集上部件分割结果样例。IoU(%)被列在每个结果下面用于参数。红色箭头代表KCNet的改进；红色圆圈代表基准数据的一些误差。使用颜色方便阅读。

## 4.3. 消融研究

在本节中，我们进一步进行了几次消融实验，研究了各种设计变化，并展示了KCNet的优势。

### 核相关的有效性

表5列出了核相关性与正态性的比较。在这个实验中，我们使用法线作为局部几何特征，将它们与坐标连接起来，并将其传递到图2a中提出的架构中。通过对协方差矩阵进行主成分分析，计算各点的正态值，得到最小方差的方向。结果表明，核相关性优于法线。

![image-20231206114159494](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231206114159494.png)

表5：在ModelNet40测试集上的消融研究

### 对称函数

对称函数能够使网络对输入排列[29]保持不变。在本实验中，我们研究了图最大池化和图平均池化的性能。如表5所示，图最大池比图平均池有一个微小的提升，并且计算速度更快，因此被采用。

### 局部结构的有效性

在表5中，我们还分别展示了通过核相关性和图最大池化学习到的局部几何结构和特征结构的影响。请注意，我们的核相关层和图最大池化层已经获得了与PointNet相当甚至更好的性能。

### 选择超参数

KCNet有几个独特的超参数：$L$、$M$和$σ$，如[第3.1节]()所述。我们在表6中报告了它们的独立影响。

![image-20231206114251937](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231206114251937.png)

表6：超参数的选择。每列仅改变对应的超参数（基准测试为黑体）。

### 稳健性测试

我们比较了我们的网络与PointNet对抗输入点云中的随机噪声的稳健性。这两个网络都在相同的训练和测试数据集上进行训练，每个对象拥有1024个点。PointNet使用与[^29]的作者相同的数据增强代码。

我们的网络是没有数据增强的训练。在测试过程中，随机选择一定数量的输入点替换为均匀分布（范围$[-1.0,1.0]$）的噪声。如图5所示，我们的网络对随机噪声更加鲁棒。当用随机噪声替换10个点后，PointNet的准确率下降了58.6%（从89.2%下降到30.6%），而我们的方法（KC+GM）仅下降了23.8%（从91.0%下降到67.2%）。此外，在实验组中，图最大池在随机噪声下最稳健。我们推测这是由于局部最大池化-邻域点在每个维数上共享最大特征而引起的，因此邻域内的随机噪声不易影响预测。这也可以解释为什么KC+GM比仅使用KC更强大。该测试显示了局部结构相对于每个点的特征所具有的优势——我们的网络学习利用相邻区域内的局部几何结构和特征结构，因此对随机噪声具有鲁棒性。

![image-20231206151630800](images/机器学习/计算机视觉/三维处理/点云/点云特征/FoldingNet/image-20231206151630800.png)

图5：在随机噪声上，KCNet v.s. PointNet。每个对象中不同数量的点被$[-1,1]$之间的均匀噪声所取代。度量值是ModelNet40测试集上的总体分类精度，结果显示KCNet对随机噪声具有更强的稳健性。GM：仅有图最大池化。KC：仅有内核相关性。KC+GM：两者都有。

# Ch05 结论

我们提出了核相关性和图池化来改进类点网络的方法。实验表明，该方法能够有效地捕获局部模式，并有效地提高了三维点云语义学习的性能。在未来，我们将把核相关推广到更高的维度，并且拥有可学习的核宽度。
