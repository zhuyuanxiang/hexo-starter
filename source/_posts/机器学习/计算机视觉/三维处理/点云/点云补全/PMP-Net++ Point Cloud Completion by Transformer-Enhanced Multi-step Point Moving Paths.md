---
title: PMP-Net++ 通过学习Transformer强化的多步骤点移动路径实现点云补全
excerpt: 点云补全关注不完整三维形状缺失部分的预测。一种常见的策略是根据不完整的输入来生成完整的形状。然而，点云的无序性会降低高质量三维形状的生成，因为在生成过程中，使用提取的潜在编码很难捕获无序点的结构和拓扑细节。因此，我们通过将补全定义为点云的变形过程来解决这个问题。具体来说，我们设计了一个全新的网络（PMP-Net++），以模拟地动的行为，通过移动每个不完整输入的点以获得一个完整的点云，其中点移动路径（Point Moving Path）的总距离满足最短约束。于是，PMP-Net++根据点移动距离的约束，为每个点预测唯一的PMP。该网络在点的级别上学习了严格的和唯一的对应关系，从而提高了预测的完整形状的质量。此外，由于移动点严重依赖于网络学习到的每个点的特征，我们就进一步推导了一个Transformer强化的表示学习网络，这显著地提高了PMP-Net++的补全性能。我们在形状补全方面进行了全面的实验，并且进一步探索了在点云的上采样方面的应用，这证明了PMP-Net++的比最先进的点云补全/上采样方法有了显著的改进。
categories:
  - 点云补全
tags:
  - 点云补全
  - 地动距离
  - 点移动路径
  - PCN
  - Completion3D
  - Transformer
  - 上采样
date: 2023-11-21
updated: 2023-11-18
toc: true
typora-root-url: D:\Projects\Github\zhuyuanxiang\hexo_pages\hexo-starter\source\_posts\
---

# PMP-Net++ 通过学习Transformer强化的多步骤点移动路径实现点云补全

Wen X, Xiang P, Han Z, et al.  PMP-Net++: Point cloud completion by transformer-enhanced multi-step  point moving paths[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022, 45(1): 852-867.

[PDF 下载链接](https://arxiv.org/pdf/2202.09507)

[原始代码](https://github.com/diviswen/PMP-Net)

# 摘要

点云补全关注不完整三维形状缺失部分的预测。一种常见的策略是根据不完整的输入来生成完整的形状。然而，点云的无序性会降低高质量三维形状的生成，因为在生成过程中，使用提取的潜在编码很难捕获无序点的结构和拓扑细节。因此，我们通过将补全定义为点云的变形过程来解决这个问题。具体来说，我们设计了一个全新的网络（PMP-Net++），以模拟地动的行为，通过移动每个不完整输入的点以获得一个完整的点云，其中点移动路径（Point Moving Path）的总距离满足最短约束。于是，PMP-Net++根据点移动距离的约束，为每个点预测唯一的PMP。该网络在点的级别上学习了严格的和唯一的对应关系，从而提高了预测的完整形状的质量。此外，由于移动点严重依赖于网络学习到的每个点的特征，我们就进一步推导了一个Transformer强化的表示学习网络，这显著地提高了PMP-Net++的补全性能。我们在形状补全方面进行了全面的实验，并且进一步探索了在点云的上采样方面的应用，这证明了PMP-Net++的比最先进的点云补全/上采样方法有了显著的改进。

# Ch01 简介

点云是一种广泛使用的三维形状表示方法，可以很容易地通过深度摄像机或其他3D扫描设备获得。由于三维扫描设备的视角或遮挡的局限性，原始点云通常是稀疏的和不完整的[^1]。因此，对于下游的3D计算机视觉应用，如分类[^2]、[^3]、[^4]、[^5]、[^6]、[^7]、分割[^8]、[^9]等视觉分析[^10]，通常需要一个形状补全/合并过程来生成三维形状中缺失的区域。

在本文中，我们关注由点云表示的三维对象的补全任务，其中缺失部分是由于扫描仪视角的自遮挡造成的。以往的方法大多将点云补全表述为点云生成问题[^1]、[^11]、[^12]、[^13]，其中通常采用编解码器框架从输入的不完全点云中提取隐式编码，并将提取的隐式编码解码为完整的点云。得益于基于深度神经网络的点云学习方法，在这条线上的点云补全方法在过去几年获得了巨大的进展[^1][^13]。然而，利用深度神经网络生成点云仍然是一项困难的任务，因为点云的无序性使得生成模型难以捕获离散点之间详细的拓扑或结构[^13]。因此，基于点云补全的生成模型的性能仍然不理想。

为了提高点云补全性能，本文提出了一种新的深度神经网络PMP-Net++，从一个新的角度来阐述点云补全任务。与直接预测三维空间中所有点的坐标的生成模型不同，PMP-Net++学习将点从源三维形状移动到目标三维形状。通过点移动过程，PMP-Net++学习了源点云与目标点云之间的点级别的对应关系，从而捕获了两个点云之间的拓扑细节和结构关系。另一方面，将点从源移动到目标存在许多可能的解，这将使网络难以很好地训练。因此，为了鼓励网络学习一个独特的最优规划的点移动路径，我们从地动距离（Earth Mover's Distance, EMD）中获得灵感，提出了正则化的Transformer强化的点移动路径网络（Point Moving Path Network, PMP-Net++），该网络的约束为所有点的移动距离（Point Moving Distance, PMD），这个约束保证了源点云与目标点云之间路径规划的唯一性。

图1给出了一个详细的说明。以完成短线$AB$到长线$A'B'$的任务为例，基于生成的神经网络旨在预测$A'B'$的坐标，通常是优化通过倒角距离（CD）或地动距离（EMD）。然而，由于点云数据的离散性，目标矩阵可以说明线$A'B'$拥有多重规划，所有这些规划都将满足CD和EMD的最小损失约束。因此，网络存在多个最优目标，不能引导网络学习短线$AB$和长线$A'B'$之间的详细形状对应关系。相比之下，基于形状变形的神经网络可以在路径约束的指导下，在源输入（$AB$）和目标输出（$A'B'$）之间建立一种直接的和逐个点的对应关系，原因见图1(c)。在网络的输入和输出之间路径约束产生效果，它正则化了点的移动路径，并优化了网络，以预测每个点的唯一位移。在这种情况下，网络的输出是位移，它与每个起点和终点都有局部相关。另一方面，基于生成的网络的输出是坐标矩阵表示的线$A'B'$，其唯一的有监督的源是整体形状约束的CD/EMD，它在输出和目标地面真实值之间生效。

![image-20231121180149897](/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%B8%89%E7%BB%B4%E5%A4%84%E7%90%86/%E7%82%B9%E4%BA%91/%E7%82%B9%E4%BA%91%E8%A1%A5%E5%85%A8/images/PMP-Net++%20Point%20Cloud%20Completion%20by%20Transformer-Enhanced%20Multi-step%20Point%20Moving%20Paths/image-20231121180149897.png)

图1：基于生成的方法与基于变形的方法之间的差异，其中的任务是完成一个短线$AB$到一个长线$A'B'$（参见(a)和(b)），进一步说明了路径约束与广泛使用CD/EMD之间的差异（参见(c)）。

此外，为了预测点的移动路径更加准确，我们提出了一种在多尺度搜索半径下连续细化点移动路径的多步骤路径搜索策略。具体来说，如图2所示，路径搜索以从粗到细的方式重复执行多个步骤。每一步都将考虑到前面预测的路径，然后根据前面的路径规划其下一个移动路径。为了记录和聚合点移动路径的历史信息，我们受门控循环单元（GRU）的启发，提出了一种新的循环路径聚合（RPA）模块。它可以记住和聚合每个点的路线序列，并将之前的信息与点的当前位置结合起来，以预测下一步移动的方向和长度。通过逐步减少搜索半径，PMP-Net++可以一致地为每个点细化一个越来越精确的路径，从不完整点云上的原始位置移动到完整点云上的目标位置。

![image-20231121180439909](/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%B8%89%E7%BB%B4%E5%A4%84%E7%90%86/%E7%82%B9%E4%BA%91/%E7%82%B9%E4%BA%91%E8%A1%A5%E5%85%A8/images/PMP-Net++%20Point%20Cloud%20Completion%20by%20Transformer-Enhanced%20Multi-step%20Point%20Moving%20Paths/image-20231121180439909.png)

图2：基于从粗到细的搜索半径使用多个步骤的路径搜索。PMP-Net++通过三步移动点$A$到点$A'$，每一步都减少其搜索半径，并且回顾考虑移动历史，以决定下一个移动的位置

本文提出的PMP-Net++是对我们最新工作PMP-Net [^14]的增强扩展。我们发现，在移动过程中学习到的点特征在预测高质量的完整形状中起着关键作用。而PMP-Net中使用的基于PointNet++ [^15]的骨架不能提供更多的鉴别点特征，因为其基于最大池的特征聚合策略[^4]。因此，受最近Transformer在点云表示学习[^16]中成功应用的启发，我们在PMP-Net++中引入了一种新的基于Transformer的框架，以增强网络学习到的点特征，目标是预测每个点更准确的位移。总之，我们工作的主要贡献可以总结如下。

- 我们提出了一种新的点云补全任务网络，称为PMP-Net++，可以将不完整形状上的每个点移动到完整形状上的对应点的位置，以实现高精度的点云补全。与以往的生成补全方法相比，PMP-Net++能够通过点移动路径来学习和预测点级别的对应关系，学习不完整形状和完整形状之间拓扑细节和结构关系。
- 我们提出在不完整的点云和完整的点云之间，利用地动距离约束来正则化网络，从而学习一个唯一的点移动距离规划。于是，网络不会被移动点的多个解所混淆，并在原始点云和目标点云之间最终预测出一个有意义的点级别的对应关系。
- 我们建议以从粗到细的方式搜索具有多步骤的点移动路径。每个步骤都将使用提出的循环路径聚合（RPA）模块，从而通过由前一步路径和当前位置聚合的信息来决定下一步的规划。
- 与我们最新的PMP-Net相比，我们在PMP-Net++中进一步引入了一个Transformer强化的点云网络，以改进点特征学习。我们在Completion3D[^13]和PCN [^12]数据集上进行了全面的实验，并进一步探索了点云上采样应用，所有这些都证明了PMP-Net++相比最先进的点云补全/上采样方法（包括我们最新的PMP-Net）有了显著的改进。

# Ch02 相关工作

在三维重建[^17]、[^18]、[^19]、[^20]和表示学习[^21]、[^22]、[^23]、[^24]中的深度学习技术促进了三维形状补全的研究，补全方法大致可分为两类：(1)传统的三维形状补全方法[^25][^26][^27][^28]通常公式化手搓特征，如：表面平滑性或对称轴来推断缺失的区域，而其他一些方法[^29][^30][^31][^32]则考虑大规模三维形状补全数据集的辅助，通过执行搜索找到类似的补丁来填补不完整的三维形状。(2)另一方面，基于深度学习的方法[^33]、[^34]、[^35]、[^36]利用其强大的表示学习能力，从不完整的输入形状中提取几何特征，并根据所提取的特征直接推断出完整的形状。与传统的补全方法相比，这些可学习的方法不需要预定义的手搓特征，并且可以更好地利用大尺度补全数据集中丰富的形状信息。所提出的PMP-Net++也属于深度学习领域，其中沿此方向的方法可以进一步分类和细化如下。

## 2.1. 体素辅助的形状补全

卷积神经网络（CNN）的表示学习能力已广泛应用于二维计算机视觉研究中，近年来有关二维图像修复的应用研究风起云涌。2D CNN在[^37]、[^38]、[^39]领域的成功可以直接扩展到三维空间，从而将其借鉴为三维形状补全的直观思想。近年来，人们提出了几种基于3D CNN结构的体积辅助形状补全方法。请注意，我们使用术语“体素辅助”来描述这类方法，是因为3D体素通常不是网络的最终输出。相反，预测的体素将被进一步细化，并转换为其他表示形式，如网格[^11]或点云[^40]，以产生更精细的3D形状。因此，体素更像是一个中间辅助工具，以帮助完成网络推断出完整的形状。沿着这个思路出现的值得注意的工作，如3D-EPN [^11]和GRNet [^40]已经提出以粗到细的方式重建完整的3D体素。他们首先在编码器-解码器框架下使用三维CNN预测一个粗糙的完整形状，然后使用从一个补全形状数据集[^11]中选择相似的补丁来细化输出，或根据输出的体素[^40]进一步重建点云的细节。此外，也有一些研究考虑使用纯粹的体素数据来实现形状补全任务。例如，Han等人[^41]提出通过同时推断全局结构和局部几何形状，来预测补全形状的细节，从而直接生成高分辨率的三维体素形状。Stutz等人[^42]提出了一种基于变分自动编码器的方法来在弱监督下补全三维体素。尽管3D CNN在特征学习方面令人动心，但输入体素数据的分辨率的立方计算成本，使得处理细粒度形状[^1]变得困难。

## 2.2. 基于点云的形状补全

近年来，基于点云的形状补全[^1]、[^13]、[^43]、[^44]任务越来越受到关注。由于点云是许多三维扫描设备的直接输出形式，而且点云的存储和处理需要的计算成本比体素数据低得多，因此最近的许多研究考虑对三维点云进行直接补全。从点云表示学习[^15]、[^45]的改进中可以看出，之前的方法，如：TopNet [^13]、PCN [^12]和SA-Net [^1]将解决方案制定为编码器-解码器框架下的生成模型。他们采用PointNet [45]等编码器从不完整点云中提取全局特征，并使用解码器根据提取的特征推断完整点云。与PCN [^12]相比，TopNet [^13]改进了解码器的结构，以便在一个根树架构[^13]中隐式地建模和生成点云。SANet [^1]进一步通过跳跃注意机制将不完整形状的详细几何信息传递为完整形状。其他值得注意的方法，如：RL-GAN-Net[^34]，Render4Completion[^44]和VRCNet [^46]，专注于对抗性学习和变分自动编码器的框架，以提高生成的完整形状的真实性和一致性。近年来，三维形状的逐步细化（如：CRN [^47]，PF-Net [^35]）已成为点云补全研究中的一个流行思想，因为它可以帮助网络生成具有结构细节的三维形状。

综上所述，上述方法大多是点云补全任务的生成式解决方案，不可避免地受点云的无序性影响，使得使用生成式解码器重建结构或者拓扑细节变得困难。因此，为了避免预测无序数据的问题，PMP-Net++使用了一种不同的方法来重建完整的点云，它学习从初始输入中移动所有的点，而不是直接从隐式编码中生成最终的点云。

PMP-Net++的思想也与三维形状变形[^48]的研究有关，它[^48]主要考虑了一步变形。然而，不完整形状和完整形状之间的变形更具挑战性，这需要在没有任何其他先验信息的情况下，推断缺失区域中完全未知的几何形状。相比之下，我们建议采用多步搜索来鼓励PMP-Net++对缺失区域推断出更详细的几何信息，以及通过正则化点移动距离，以保证多步推理的有效性。

# Ch03 PMP-Net的架构

提出的PMP-Net++的（概述见图4）由三部分组成：

1. 使用编码器抽取点云特征
2. 使用特征传播模块（Feature Propagation Module，FP-Module）预测每个点的点移动路径
3. 使用RPA模块循环地融合和聚集当前步骤的点特征与前一步的路径信息

每个部分的细节描述如下。

![image-20231121180758767](/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%B8%89%E7%BB%B4%E5%A4%84%E7%90%86/%E7%82%B9%E4%BA%91/%E7%82%B9%E4%BA%91%E8%A1%A5%E5%85%A8/images/PMP-Net++%20Point%20Cloud%20Completion%20by%20Transformer-Enhanced%20Multi-step%20Point%20Moving%20Paths/image-20231121180758767.png)

图3：第$k$步PMD-模块的结构细节。主要由三个部分组成：(1)点云编码器；(2)特征传播模块（FP-模块）抽取每个点的特征；(3)RPA模块用于循环学习和遗忘前一步的路径搜索信息

![image-20231121181002766](/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%B8%89%E7%BB%B4%E5%A4%84%E7%90%86/%E7%82%B9%E4%BA%91/%E7%82%B9%E4%BA%91%E8%A1%A5%E5%85%A8/images/PMP-Net++%20Point%20Cloud%20Completion%20by%20Transformer-Enhanced%20Multi-step%20Point%20Moving%20Paths/image-20231121181002766.png)

图4：基于由粗到细的搜索半径使用多个步骤的路径搜索。PMP-Net++通过三步移动点$A$到点$A'$，每一步减少其搜索半径，并且回顾考虑移动历史，以决策下一个移动的位置。

## 3.1. 预测点的位移

### 3.1.1. 多步骤架构

提出的PMP-Net++（概述见图4）给定了一个输入点云$P=\{\mathbf{p}_i\}$和一个目标点云$P'=\{\mathbf{p}'_ j\}$，其目标是预测一个位移向量集$\Delta P=\{\Delta\mathbf{p}_i\}$，基于该向量集可以将每个点从$P$移动到$P'$的位置，使$\{(\mathbf{p}_i+\Delta\mathbf{p}_i)\}=\{\mathbf{p}'_j\}$。PMP-Net++总共用$k=3$步移动每个点$\mathbf{p}_i$。步骤$k$的位移向量用$\Delta\mathbf{p}_i^k$表示，所以$\Delta\mathbf{p}_i=\sum_{k=1}^3\Delta\mathbf{p}_i^k$。对于步骤$k$，网络将最后一步$k−1$的变形点云$\{\mathbf{p}^{k−1}_i\}=\{\mathbf{p}_i+\sum^{k−1}_{j=1}\Delta\mathbf{p}^j_i\}$作为输入，并根据输入点云计算新的位移向量。因此，预测的形状将一步步地细化，最终产生一个高质量的完整形状。

### 3.1.2. Transformer增强的位移预测

在第$k$步，为了预测每个点的位移向量$\Delta\mathbf{p}^k_i$，我们首先从点云中提取每个点的特征。在之前的实现PMP-Net [14]时，首先采用PointNet++[15]基础框架提取输入的三维形状的全局特征，然后使用特征传播模块传播将全局特征传播到三维空间的每个点上，最终为每个点$\mathbf{p}^k_i$输出其对应的特征$\mathbf{h}^{k,l}_i$。在PMP-Net++中，我们采用最近成功实现的Transformer[49]来强化由PointNet++学习得到的点特征，在其中我们遵循Point Transformer[16]的实践成果，并在每个PointNet++的集合抽象（Set Abstraction, SA）层之间添加一个额外的Transformer模块。PMP-Net和PMP-Net++的结构细节之间的比较如图5所示。具体来说，在PMP-Net++中，由上一层集合抽象（SA）层学习到的局部特征（为了方便表示为$\{x_{in}\}$）被输入到后序的Transformer模块。在Transformer模块中，$\{x_{in}\}$作为计算自注意的“键”（key）和“查询”（query），根据自注意力计算，通过多个MLP和逐元素的操作得到一个新的局部特征集合$\{x_{out}\}$。请注意，图5(c) 中编码的位置用于引导网络学习不同局部特征之间的空间关系。我们遵循与之前的工作[16]相同的做法，采用可学习的位置编码，学习参数依赖于两点$p_i$和$p_j$之间的三维坐标：
$$
\begin{equation}
\epsilon=\text{MLP}(p_i-p_j|\theta_\epsilon)
\end{equation}
$$
由于我们的实验实现应用了三个层次的特征传播来层次化地生成每一个点的特征（见图3），我们在$\mathbf{h}^{k,l}_i$使用上标$k$来表示步骤，下标$l$来表示层。然后，将每个点的特征$\mathbf{h}^{k,l}_i$与随机噪声向量$\hat{x}$连接，根据[48]所述该噪声向量可以对点产生微小的干扰，迫使它离开原来的位置。然后，将步骤$k$和第$3$级的最后一个点特征$\mathbf{h}^{k,3}_i$输入多层感知器（MLP），后接一个$\tanh$激活函数，生成一个三维向量作为点$\mathbf{p}^k_i$的位移向量$\Delta\mathbf{p}^k_i$：
$$
\begin{equation}
\Delta\mathbf{p}^k_i=\tanh(\text{MLP}([\mathbf{h}^{k,3}_i:\hat{x}]))
\end{equation}
$$
其中冒号“:”表示拼接操作。

![image-20231122101307284](/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%B8%89%E7%BB%B4%E5%A4%84%E7%90%86/%E7%82%B9%E4%BA%91/%E7%82%B9%E4%BA%91%E8%A1%A5%E5%85%A8/images/PMP-Net++%20Point%20Cloud%20Completion%20by%20Transformer-Enhanced%20Multi-step%20Point%20Moving%20Paths/image-20231122101307284.png)

图5：在PMD模块中使用的编码器架构。而且，我们还对比了先前的工作与Transformer的结构细节。

### 3.1.3. 不同步骤之间的循环信息流

前面的移动的信息对于网络决定当前的移动是至关重要的，因为前面的路径可以用来推断单个点的最终目的地的位置。此外，这些信息可以引导网络找到下一步移动的方向和距离，防止网络在点移动路径搜索的多个步骤中改变目的地。为了实现这一目标，我们建议在每一步和每一层的特征传播模块之间使用一个特定的RPA单元，用于记忆之前路径的信息，并推断每个点的下一个位置。如图3所示，RPA模块（步骤$k$，层$l$）将第$k-1$层的输出作为输入，然后输出$\mathbf{f}^{k,l-1}_i$，并将其与特征$\mathbf{h}^{k-1,l}_i$（步骤$k-1$，层$l$）合并，输出当前层的特征$\mathbf{h}^{k,l}_i$（步骤$k$，层$l$）：
$$
\begin{equation}
\mathbf{h}^{k,l}_i=\text{RPA}(\mathbf{f}^{k,l-1}_i,\mathbf{h}^{k-1,l}_i)
\end{equation}
$$
RPA模块的结构细节如上所述。

## 3.2. 循环路径聚合

路径聚合模块的详细结构如图6所示。之前的点移动路径可以看作是顺序数据，其中每个移动的信息应该在这个过程中被选择性地记忆或遗忘。遵循这个想法，我们从循环神经网络获得灵感，模仿门控制循环单元（GRU）计算更新门$z$和重置门$r$从而编码和遗忘信息，计算的依据是点特征$\mathbf{h}^{k-1,l}_i$（步骤$k-1$，层$l$）和点特征$\mathbf{f}^{k,l-1}_i$（步骤$k$，层$l-1$）。两个门的计算可以表示为：
$$
\begin{align}
z=\sigma(W_z[\mathbf{f}^{k,l-1}_i:\mathbf{h}^{k-1,l}_i]+\mathbf{b}_z)\\
r=\sigma(W_r[\mathbf{f}^{k,l-1}_i:\mathbf{h}^{k-1,l}_i]+\mathbf{b}_r)
\end{align}
$$
其中，$W_z,W_r$是权重矩阵，$\mathbf{b}_z,\mathbf{b}_r$是偏差。$\sigma$是`sigmoid`激活函数，用于预测一个$0$到$1$之间的值，表示信息通过门的比例。冒号“:”表示两个特征的拼接。

与标准GRU不同，在GRU中计算当前步骤$k$的输出特征$\mathbf{h}^{k,l}_i$时，更强调对先前信息的保留，在RPA中则更重视对当前输入信息的保留。RPA计算输出特征$\mathbf{h}^{k,l}_i$的公式为：
$$
\begin{equation}
\mathbf{h}^{k,l}_i=z\odot\hat{\mathbf{h}}^{k,l}_i+(1-z)\odot\mathbf{f}^{k,l-1}_i
\end{equation}
$$
其中，$\hat{\mathbf{h}}^{k,l}_i$是当前步骤的中间特征，它包含了从过去开始的所有保留信息，是依据当前输入特征计算得到的，公式如下：
$$
\begin{equation}
\hat{\mathbf{h}}^{k,l}_i=\varphi(W_h[r\odot\mathbf{h}^{k-1,l}_i:\mathbf{f}^{k,l-1}_i)]+\mathbf{b}_h)
\end{equation}
$$
其中，$\varphi$是`relu`激活函数。

使用$\hat{\mathbf{h}}^{k,l}_i$而不是$\mathbf{h}^{k-1,l}_i$与$\mathbf{f}^{k,l-1}_i$融合的原因是，与标准的RNN单元相比，在决策下一步移动时点的当前位置影响更大。特别是，当RPA模块需要忽略对当前位置决策不重要的先前信息时，等式(6)可以简单地将更新门$z$设置为零向量从而遗忘所有历史，从而使RPA模块完全关注于当前输入$\mathbf{f}^{k,l-1}_i$的信息。

![image-20231122112551183](/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%B8%89%E7%BB%B4%E5%A4%84%E7%90%86/%E7%82%B9%E4%BA%91/%E7%82%B9%E4%BA%91%E8%A1%A5%E5%85%A8/images/PMP-Net++%20Point%20Cloud%20Completion%20by%20Transformer-Enhanced%20Multi-step%20Point%20Moving%20Paths/image-20231122112551183.png)

图6：RPA模块（步骤$k$，层$l$）的详细结构。

## 3.3. 优化后的搜索用于唯一路径

### 3.3.1. 最小化移动距离

如图7所示，点云的无序性质导致输入形状变形为目标形状容许出现多个解，并且对变形后的形状及其基准数据的直接约束（如倒角距离）也不能保证输入点集与目标点集之间建立的对应关系的唯一性。除此之外，网络还会被点移动的多种解所混淆，从而导致无法捕捉到不完整形状和完整形状之间的详细拓扑和结构关系。为了建立一个独特的和有意义的输入点云和目标点云之间的逐点对应关系，我们从地动距离[50]获得灵感，提出了基于所有点移动距离的约束来训练PMP-Net++从而学习在原始点云和目标点云之间的路径规划$\phi$。具体来说，给定源点云$\hat{X}=\{\hat{x}_i|i=1,2,3,\dots,N\}$和目标点云$X=\{x_i|i=1,2,3,\dots,N\}$，遵循EMD学习路径规划$\phi$，要求满足以下约束：
$$
\begin{equation}
\mathcal{L}_{\text{EMD}}(\hat{X},X)=
\min_{\phi:\hat{X}\rightarrow X}\frac1{\hat{X}}
\sum_{\hat{x}\in\hat{X}}\|\hat{x}-\phi(\hat{x})\|
\end{equation}
$$
在等式中(8)，$\phi$被看作是使$\hat{X}$和$X$中对应点之间的平均距离最小的双射。

这里，两个点云之间的唯一对应关系可以通过EMD（等式(8)）的定义来保证，解释如下：(1)将点云A变形为点云B（点数相同的点）等价于在点云A与B之间建立双射$\phi$；(2)当点位移之和达到最小值时，双射$\phi$是唯一的；(3)优化等式化(8)是为了最小化点位移的和，这将产生一个唯一的对应关系，遵循由输入点云确定的数据顺序。

![image-20231122142535275](/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%B8%89%E7%BB%B4%E5%A4%84%E7%90%86/%E7%82%B9%E4%BA%91/%E7%82%B9%E4%BA%91%E8%A1%A5%E5%85%A8/images/PMP-Net++%20Point%20Cloud%20Completion%20by%20Transformer-Enhanced%20Multi-step%20Point%20Moving%20Paths/image-20231122142535275.png)

图7：当输入点云（绿）变形为目标点云（红）时多个解的说明。PMD约束保证了输入点云与输出点云在点层级上对应关系的唯一性(a)，并且过滤了移动点时的其他解(b)

根据等式(8)，由网络建立的双射$\phi$应实现将点从输入形状移动到目标形状的最小移动距离。然而，即使输入点云和目标点云之间的对应关系是唯一的，但源点和目标点之间仍然存在不同的路径，如图8所示。因此，为了鼓励网络学习一个最优的点移动路径，我们选择最小化点移动距离损失（$\mathcal{L}_{PMD}$），它是通过PMP-Net中所有三步移动后得到的所有位移向量$\{\Delta\mathbf{p}^k_i\}$输出的和。点移动距离（Point Moving Distance, PMD）损失的公式为：
$$
\begin{equation}
\mathcal{L}_{PMD}=\sum_k\sum_i\|\Delta\mathbf{p}^k_i\|_2
\end{equation}
$$
等式(9)比EMD约束更为严格。它不仅要求所有点的整体位移达到最短的距离，而且还限制了每一步的点移动路径为最短的距离。因此，在每一步中，我们将鼓励网络按照之前的方向搜索新的路径，如图8所示，这将减少冗余的移动决策，提高搜索效率。

### 3.3.2. 多尺度搜索路径

PMP-Net++以从粗到细的方式搜索点移动路径。对于每一步，PMP-Net++将按$10$的幂次方减少移动的最大步幅，即对于步骤$k$，在等式(2)中计算的位移$\Delta\mathbf{p}^k_i$被限制为$10^{-k+1}\Delta\mathbf{p}^k_i$。这使得网络在训练期间能够更快地收敛。同时，减少的搜索范围将保证网络在下一步不会推翻其在上一步中做出的决定，特别是对于长距离移动。因此，它可以防止网络在路径搜索过程中做出冗余决策。

## 3.4. 稠密点云补全的扩展

点云的变形不能直接用于增加点的数量。因此，输入点云和输出点云必须具有相同的分辨率。这种基于PMP-Net++的变形特性在处理不同点数的三维形状时可能会是一个问题。为了解决这一问题，我们通过向每一步的输入中添加噪声，将PMP-Net++扩展到密集的点云补全场景。为了实现这一目标，在步骤$k$中，将输入点云$\{\mathbf{p}^k_i\}$与符合标准正态分布$N(0,1)$的噪声向量$\hat{n}$拼接起来：
$$
\begin{equation}
\{\mathbf{p}^k_i\}\leftarrow\{[\mathbf{p}^k_i:\hat{n}]\},\hat{n}\sim N(0,1)
\end{equation}
$$
因此，每次输入网络时，点云都与之前的数据不同。然后，通过多次变形点云，并将变形结果重叠在一起，可以得到比原始输入点更多的稠密点云。

请注意，等式(2)中的噪声是使这些点远离其在三维空间中的原始位置，而在每一步的输入中使用的噪声的是改变最终的变形结果。

### 更多讨论

通常在基于生成的方法中，扩展到稠密点云补全的关键思想是增加输入点的数量。对于PMP-Net，我们复制输入点，并将之与随机噪声进行拼接，以增加点的数量。而对于其他方法，如：TopNet [13]和PCN [12]，它们还需要复制特征，并与二维网络编码拼接，以增加输入点。PMP-Net++与其他方法的根本区别在于，复制操作是在网络的不同阶段完成的。

请注意，如果我们简单地重复点坐标，并不带额外操作地移动它们，那么重复的点将产生完全相同的位移。因此，为了解决这个问题，在PMP-Net++中，我们在每个点特征上添加噪声，以迫使它们移动到不同的地方。因为噪声增强的输入点在运动之前已经位于不同的空间位置，这些复制结果将不会被移动到相同的目标点。

## 3.5. 训练的损失函数

利用倒角距离（CD）和地动距离（EMD），通过完整的基准点云正则化变形后的形状。遵循与等式(8)中相同的符号，倒角距离（CD）的定义如下：
$$
\begin{equation}
\mathcal{L}_{CD}(X,\hat{X})=
\sum_{x\in X}\min_{\hat{x}\in\hat{X}}\|x-\hat{x}\|+
\sum_{\hat{x}\in\hat{X}}\min_{x\in X}\|\hat{x}-x\|
\end{equation}
$$
用于训练的完整损失函数定义如下：
$$
\begin{equation}
\mathcal{L}=\sum_k\mathcal{L}_{CD}(P^k,P')+\mathcal{L}_{PMD}
\end{equation}
$$
其中，$P^k$表示第$k$步的点云输出，$P'$表示目标完整点云。请注意，找到最优$\phi$的代价非常高。在实验中，我们遵循[^12]简化算法来估计$\phi$的近似值。

